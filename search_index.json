[["index.html", "BIOL 480/580Q: Ecological Modeling (Fall 2024) 1 Introduction 1.1 About this ‘book’ 1.2 Why this course? 1.3 Getting comfortable with R and RStudio 1.4 How to approach this course", " BIOL 480/580Q: Ecological Modeling (Fall 2024) Eliza Grames 1 Introduction 1.1 About this ‘book’ These are my notes to accompany lectures for BIOL 480Q/580Q: Ecological Modeling (Fall 2024). They are not meant to be a textbook by any means, but rather a pretty casual introduction to statistics and modeling in R relevant to ecologists. Because the course is cross-listed as a graduate and undergraduate course, I assume many folks are coming to it with minimal background in probability, statistics, or coding, so this should hopefully be a pretty gentle introduction. Also, I am writing it up as we go along in the course this semester, so more sections will be added as we go along. I will do my best to tell you which section(s) we will cover each week but we may spill over if we get through something quickly, or spend more time on some concepts than I anticipate. 1.2 Why this course? Just before I started graduate school nearly a decade ago, a survey found that most early career ecologists lacked adequate quantitative training and 75% were not satisfied with their own understanding of ecological modeling. Ecology has always been heavily rooted in mathematics, and has become increasingly quantitative in recent decades, but graduate training has lagged behind that trend. I was lucky to have taken an Ecological Modeling course taught by two brilliant quantitative ecologists (and excellent human beings) - Morgan Tingley and Robi Bagchi - during my PhD at the University of Connecticut. This course is inspired by theirs, and similarly aims to train the next generation of quantitative ecologists. Figure from Barraquand et al. (2014) showing the relationship between early career ecologists’ satisfaction with their understanding of mathematical models and how involved they are in ecological modeling. One of the reasons I teach this course is because I think it is really important for students to really understand their data and how to properly analyze it to address their hypotheses. Too often, the way statistics is taught (including how I was first introduced to it as an undergrad) is very rigid, with strict rules to follow, assumptions to be met, and prescribed ways of doing things. Many of us have come across guidelines like these charts for how to pick the right statistical test for your data. I also think that this approach to learning statistics instills a wariness about doing analyses ‘wrong’, which can turn many students away from statistics and modeling. I have absolutely no data to back this up, but I suspect the rigid way in which statistics is taught is one reason for a gender gap in quantitative ecology. Gender stereotypes can cause girls to internalize that they are ‘bad at math’ at an early age, which combined with a fear of not getting the ‘rules’ of statistics right, could be one reason why only 4% of early career women in ecology indicate being very involved in ecological modeling compared to 10% of men. Gender differences in how PhD students and postdocs in North America rate their involvement in ecological modeling (5 being high, 1 being low). Figure derived from supplemental materials in Barraquand et al. (2014). Gender differences in how PhD students and postdocs in North America rate their level of comfort in using equations for ecological modeling (5 being high, 1 being low). Figure derived from supplemental materials in Barraquand et al. (2014). It is also really important for students to develop quantitative skills for their future employment prospects. For graduate students aiming for academic careers, quantitative skills can help them secure permanent positions. 40% of faculty job listings in ecology and related fields require some level of quantitative skills, and 21% require “strong” quantitative skills. Many industry and government research positions also require quantitative skills, including data analysis, visualization, and modeling. Perhaps most importantly, learning how to code and build models opens up a whole new world of possibilities for asking interesting questions. More often than not, we address questions using the tools we have at our disposal (i.e. if you’ve got a hammer, everything looks like a nail). The type of statistical tests that students often learn are inadequate to deal with many of the questions and hypotheses in ecology and related fields. Instead of being constrained by what statistical tests will be possible with the data or cramming the data into a test that does not really fit, students can learn to build models that truly address their research questions, opening up the possibility for more novel hypotheses. 1.3 Getting comfortable with R and RStudio We will be using almost exclusively R, accessed through RStudio, for this course. R is a programming language, so you will not access it directly. Instead, we will use RStudio, which is a graphical user interface (GUI) that makes it a lot easier to use R for your analyses. Install R. Follow the instructions for your operating system here: https://cran.r-project.org/ Install RStudio. Follow the instructions for your operating system here: https://posit.co/download/rstudio-desktop/ Sometimes, your computer will add a shortcut icon to access R directly which will pull up an older looking GUI. Do not use it. Always use RStudio to interact with R (…unless you’re running shell scripts or some other niche purposes, but if you’re doing that, you probably aren’t reading instructions on how to access R). The logo on the right is RStudio, which is the one you want. When you open RStudio for the first time, it should look something like this: In the upper left is the Source pane. This is where you will open scripts, edit code, run analyses, type notes, etc. To use a cooking analogy, the Source pane is your recipe book. You can always come back to it, make little changes, or leave notes about what to fix for next time. But it is not the actual meal, just instructions for making a meal. In the bottom left is the Console. The Console is the process of cooking. It can be done following the recipe by running chunks of code from the Source, or you can wing it. If you decide to ‘cook’ on the fly by running code directly from the Console, you will save no notes on what on earth you put into the meal, you cannot go back and fix things easily, and you cannot share your recipe with anyone else. You also might not remember the order in which you did things. So if what you did worked and your ‘meal’ comes out great, you will not know what you did to make that happen so it is not reproducible. The only real use for the console is for code that you only want to run once, e.g. foundational things like installing packages; to really overcook this analogy (ha), you only install a kitchen counter once, not at the start of every recipe. More on packages later. In the upper right is the Environment (along with a few other tabs that you’ll use far less). This is the current version of your meal, i.e. if you’re making a stew, this is everything that is currently in the pot or mise en place and ready to go. It is objects (think of these as ingredients) that you have created with your cooking and are now available to either eat or combine with other objects (i.e. ingredients). In the bottom right are your Files, Plots, and Help - all of which you will use frequently (and other tabs you will use far less often). Think of these as things that are available to you, but external to your current kitchen counter and stew. The Files tab will let you browse files on your computer; think of it is your kitchen where you can get more resources for your meal if you are in your current working directory (more on directories later), or your house if you browse for other files on your computer. Your Plots tab is the equivalent of taking a picture of the current version of your meal, either to be able to visualize what you’ve currently got, or to share with others. The Help tab is exactly what it sounds like - the person you call to ask how to use the stand mixer (i.e. the function) and what to put into it (i.e. the arguments passed to the function) to make the perfect waffle. It’s up to you to know that you want to use the stand mixer to make waffles in the first place. 1.3.1 Some terms and definitions Since that analogy is now burnt to a crisp, let’s unpack some of the extra terms in there and what they are because you will use them frequently in this course. 1.3.1.1 Packages Packages are a collection of functions designed to work together to accomplish some specific outcome. Many packages are hosted on CRAN, however, you can also find R packages on repositories like GitHub. One way to find packages is through CRAN Task Views (e.g. these are all packages associated with meta-analysis https://cran.r-project.org/web/views/MetaAnalysis.html), but more often you’ll just Google what you want to do and find a package that way. To install a package from CRAN, you can use the install.packages() function with the name of the package inside the open parentheses in quotes (e.g. install.packages(\"lme4\")) to install the lme4 package which is useful for linear models. To install packages from other sources, follow the package developer’s instructions. 1.3.1.2 Objects Objects are what are in your environment. They can take lots of different forms, have different classes, etc. Most objects are created by using the assignment operator &lt;- to pass the output of some code to a named object. We will talk about different types of objects more during the semester as we encounter them. 1.3.1.3 Working directory Your Files tab lets you see two things: files, and directories. Directories are the organizational structure for how you store data on your computer; you can think of them as folders for the most part, though folders are GUI ways to visualize directories and directories have a clear nested structure. You working directory is very important when coding. File paths are relative to your current working directory, so when you read files in you must know both what your current working directory is; most of the time if you get an error reading in a file, it is because the path to the file is incorrect. In the cooking analogy, you are cooking on the kitchen countertop, which is nested within the kitchen, which is nested within your house. You could move directories within the kitchen, such as moving to the sink, or you could move up several levels to go to the living room. If you try to call a file that is not in your current working directory, you will get an error. For example, if you are in the living room and tell R to pick up your cutting board, it will say it does not exist. Relative file paths are extremely useful in coding. Relative file paths begin in your current working directory. To load a file from your current directory, begin the file path with ./. To load a file from the working directory above you (i.e. if you’re working at the kitchen counter, the kitchen is the next hierarchical level above you), use ../. One way to remember the difference between one dot and two is that if the dots represent your feet, one shows where you are standing - but two means you’ve hopped somewhere new. Since directories are nested within each other, you can also combine these into longer relative paths. So, for example, if I am in the living room but I want R to go into the kitchen, then into the cabinet, and take out a cutting board, the path would be something like: ../kitchen/cabinet/cutting_board.txt. Setting your working directory. There are two ways to set your Working Directory. One is the click-and-point way, where you can go to Session &gt; Set Working Directory &gt; Choose Location which will open up your normal file browser application and you can navigate around to find where you want to set as your current working directory. This is easier when you’re starting out and getting used to directory structure and how you have your files organized, but is not reproducible so can cause headaches if you think you’re in a different directory than you are later on when running a script. A better option is to use the function setwd() with an absolute path to a directory. An absolute path is one that starts at your home directory (e.g. on Linux or Mac, ~/, on Windows typically something like C:/). For example, I might run something like setwd(\"~/Desktop/480Q\") at the start of my script, and then use relative paths throughout once I am in the directory where I have my files for analysis and where I want to save my output. 1.3.1.4 Functions Functions take input as arguments, and return output. To see what arguments can be passed to a function, and also what its output will be, you can use the Help tab to search for a function. Or, much more quickly, use a ? followed by the function name if it is in a package that is currently loaded (e.g. ?rbinom). If it is in a package that is not currently loaded, use ?? instead (e.g. ??glmer). 1.3.2 Customizing RStudio There are two modifications many people will want to make to RStudio: To change the theme, go to Tools &gt; Global Options. For example, you may prefer a dark theme if you’re coding frequently. Rearrange the panes. There is a window-like icon to the left of ‘Addins’ in the tool bar; select the drop down and you can customize which panes are in which corners. For example, I prefer to move my Console to the right so I can see my code in parallel and make the Environment tab really small because I do not need to check it frequently. 1.4 How to approach this course The goal of this course is to get you to think quantitatively i.e. recognizing that data are generated by deterministic and stochastic processes, both of which are defined by you (or your understanding of the biology of the system that you are modeling or making inference on). No one can really tell you how to model or analyze your data - there is no lookup table for ecological modeling. After this course, you should know enough to 1) be dangerous on Google / StackOverflow to find analytical / modeling approaches that match your questions and data, and 2) write or adapt code to implement those models and not be reliant on a GUI. The goal is not complete mastery of statistics, modeling, or data analysis. The goal of this course is to get you closer to becoming dangerous. 1. It is not about content mastery. This course likely requires a different mindset from what you have encountered previously, primarily because it is not about content mastery. To be a specialist in any field, including biology, there is a lot of content that you need to have under your belt, which is why many undergraduate (and graduate) courses emphasize understanding principles and knowing certain key information (e.g. you need to know population dynamics, cell division, anatomy and physiology, evolutionary principles, etc.). In contrast, this course is much more about building your toolset and learning techniques, then recombining them in new ways when faced with different modeling problems. In many ways, learning how to analyze your data, how to code, how to specify models, etc. is like learning how to cook. Think of content mastery as foraging - if you are in the woods collecting mushrooms, you must know which ones are edible and which are poisonous and how to tell the difference. Knowing what you can and cannot eat is essential background information. Modeling, on the other hand, is more like cooking - what are you going to do with what you’ve gathered? 2. It is better to know of many methods than to know a few in-depth. We have 28 hours together this semester. With that time, I could teach you one specific model each week and we could go very in-depth to know all the details, assumptions, modifications, etc. To put it in a cooking context, I could teach you one recipe each week and by the end of the course you would know how to make 14 of my favorite dishes. Or, instead, you could be exposed to many different types of food that you might like to make depending on what you’re in the mood for, covering a variety of cooking styles, techniques, and cuisines. Knowing that these dishes exist and having a general sense of what you want to make gives you many more options than just knowing how I make guacamole. 3. Not all methods are suitable for your research questions. Even knowing that there are many dishes you could make, not all of them will suit you. Some weeks the dishes won’t work for your dietary needs (e.g. maybe you never have nested data) or you will wonder why anyone would ever want to cook French food. Then, we will get to Mexican food the next week (er, I mean structural equation modeling) and it will be exactly what you were looking for. Even on weeks that are not currently relevant to your research, it is still good to know that they exist and you may encounter those methods in other ways and be glad you have a basic understanding (e.g. as a peer reviewer). 4. You can always look up how to implement a method. If you know a dish exists but you are not entirely sure how to make it, you can look up a recipe and follow along. The same is true of models. Some recipes are not exactly what you want, but if you know what you’re looking for, eventually you’ll find a good example, guide, or tutorial. And sometimes, you may go to a restaurant (i.e. a journal article) and you really like it and try to recreate it! 5. Practicing is important. The first time you try a new recipe, it may not go well, but you’ll get better with practice. And at least you tried! With coding, you will need to keep at it and you will learn new tricks the more you practice. 6. Selecting methods a priori is better than post-hoc. Sometimes, you have been handed ingredients and are trying to find a recipe that will work with what you have. For example, in a data context, your PI may give you a dataset, you may be working with historical data, or you collected the data before having a plan for analysis. In an ideal world, you know what you are going to cook in advance so you have all the ingredients that you need on hand. Figuring out which models are possible post-hoc. Planning your analyses before collecting data, "],["probability.html", "2 Probability 2.1 Bernoulli distribution 2.2 Binomial distribution 2.3 Probability mass 2.4 The Normal (Gaussian) distribution 2.5 Probability density 2.6 The Standard Normal Distribution", " 2 Probability 2.1 Bernoulli distribution If you flip a coin, what is the probability that it lands up heads? There is a 0.50 probability, or 50% chance, of flipping heads. There is also a 0.5 probability that the outcome is tails. These two probabilities have to sum to 1 because there is no chance of any other outcome. Put another way, if \\(p_h = 0.5\\), then \\(p_t = 1 - p_h = 0.5\\), where \\(p_h\\) is probability of heads and \\(p_t\\) is probability of tails. Now, instead of thinking about flipping a coin as a 50/50 shot at heads or tails, think about it only in terms of succeeding at flipping heads. Each successful attempt you get a ‘1’ for a success, and each time the coin is not heads, we call it a ‘0’ for failure. Put another way, we ask the question ‘Did we flip heads?’ and if the answer is TRUE, and we use a 1 to denote that. This is actually how R encodes TRUE (T = 1) and FALSE (F = 0). We can use R to simulate a coin flip using the function rbinom(). # to leave notes to yourself in R scripts, start with a # ?rbinom # look up the Help file to find out about the arguments to the function # rbinom wants: # n, the number of observations / replicates # size, the number of draws # p the probability of success # to simulate a single coin flip, we want to take one replicate (n=1) of # one draw (size=1) from a binomial distribution with a probability of # success (heads) of 0.5 rbinom(n=1, size=1, prob=0.5) ## [1] 1 # we could keep running this line over and over # or we could increase n, the number of replicates # this gives us a vector of 100 observations of a coin flip rbinom(n=100, size=1, p=c(0.5)) ## [1] 1 1 0 0 1 0 0 0 1 0 1 0 0 1 1 1 1 0 1 0 0 0 0 1 1 0 0 0 0 1 0 1 0 0 0 0 1 ## [38] 1 1 1 1 0 0 0 1 1 1 0 1 0 0 0 0 0 0 0 0 0 1 1 0 0 0 1 1 1 1 1 1 1 0 0 1 1 ## [75] 0 0 0 0 1 0 0 0 1 1 1 0 0 0 1 1 0 1 0 1 1 1 1 1 1 0 # to visualize the outcome, we can create a histogram plotting the results # using the function hist() hist(rbinom(n=100, size=1, p=0.5)) # because the function is drawing from a probability distribution, outcomes will # be a little different every single time. If you need them to be the same, you # can set the seed with set.seed(42), or any other number The outcome we are observing, successfully flipping heads, is a random variable. A random variable is one that is drawn from a probability distribution (not in the colloquial sense of the word random). Our observed response (heads = TRUE) is drawn from a Bernoulli distribution with a probability of success of 0.5. We can write this mathematically as: \\[heads \\sim Bernoulli(p=0.5)\\] The Bernoulli distribution is a probability distribution that only yields success (1) or failure (0) based on the probability of success \\(p\\). The probability can vary depending on the process you are modeling, but the outcome will always be 0 or 1. As an ecological example, let’s take guillemot chicks. Guillemots are seabirds that nest on steep cliffsides to escape predators. They spend most of their life on the ocean, however, and as such are very awkward on land and also not great at flying. To get from the cliffside nest to the ocean for the first time, guillemot chicks have to jump and then glide/fly. If they do not make it all the way, they can attempt to run, but are vulnerable to predation. The only option for chicks is to survive or fail - making it 70% of the way to the ocean will result in failure (and, in this case, death). # let&#39;s assume chicks have a 40% chance of making it to the ocean, and there are # 100 chicks jumping from the cliff hoping to make it to the ocean so we have # 100 observations, each with one draw from the Bernoulli distribution rbinom(n = 100, size=1, p=0.4) ## [1] 1 0 0 1 0 1 0 0 0 0 1 0 0 0 1 0 1 0 0 0 1 1 0 0 0 0 0 0 0 1 1 0 0 1 1 1 1 ## [38] 0 0 1 0 0 1 0 1 1 1 1 0 0 1 1 1 0 1 1 0 0 1 1 1 1 0 0 0 1 1 1 1 0 0 0 0 0 ## [75] 0 0 0 1 0 1 0 1 1 1 0 0 1 0 1 1 1 1 0 0 0 0 0 0 1 1 # so far, we have just been running functions and the output is shown in the console # but we don&#39;t have anything in our environment # can save to an object with the &lt;- assignment operator survival &lt;- rbinom(n=100, size=1, p=0.4) # we can then do things with that saved object, like plot it hist(survival) 2.2 Binomial distribution 2.2.1 Guillemot example Guillemots typically only lay 1-2 eggs, but for the sake of our example code, let’s pretend that each pair has 3 chicks that are attempting to make it to the ocean. Each individual chick’s attempt could be modeled with a Bernoulli distribution, but if we consider it from the perspective of the breeding pair, the total number of surviving offspring is what matters. We can think of the number of chicks from each nest that make it to the ocean as the sum of their individual attempts for a single nest. This is the Binomial distribution, which is a more generalized version of the Bernoulli distribution (more accurately, the Bernoulli distribution is a special case of the Binomial distribution with one draw) in which each observation has multiple draws. # for one nest with 3 chicks and a probability of success of 0.4 rbinom(n=1, size=3, p=0.4) ## [1] 1 # we could also think about a colony with let&#39;s say 100 nests # how many chicks from each nest will make it? nest_offspring &lt;- rbinom(n=100, size=3, p=0.4) hist(nest_offspring) nest_offspring ## [1] 2 2 0 1 2 2 2 0 1 0 1 2 2 2 1 0 1 2 0 2 1 2 2 1 3 3 0 3 3 1 1 1 2 2 1 1 2 ## [38] 1 2 2 0 2 1 0 1 0 2 1 0 0 1 1 1 1 2 2 1 2 1 2 1 1 1 3 0 2 2 2 1 2 0 1 0 0 ## [75] 0 2 2 2 1 2 2 2 1 0 2 1 2 2 0 2 1 3 2 1 1 0 2 0 1 1 # what is the probability that all three chicks from a nest make it? # because we know what we fixed p to, we could multiply the probabilities 0.4^3 ## [1] 0.064 # what is the probability that none make it? (1 - 0.4)^3 ## [1] 0.216 We are able to calculate what the probability of these outcomes are because we fixed the probability of success at 0.4 and then simulated outcomes. In the real world, we rarely know what the probability of success truly is, and instead we are working backwards from our observed outcomes (i.e. our data) to estimate the probability of survival. One of the beautiful things about working with simulated data is that we know the input value, so we can test assumptions, see if we recapture our input, and play around with different model structures and know that anything that is unexpected is most likely a problem with our code or model, not the data. With discrete distributions like the Binomial, we can convert the frequency of outcomes to a proportion of the total as an estimate of probability. Dividing the frequencies by the total sample size makes it so that the total probability sums to 1. # table is a function that tallies up all the items in a vector # a vector is a type of object that is one dimension, i.e. nest_offspring is a # vector with length = 100 table(nest_offspring) ## nest_offspring ## 0 1 2 3 ## 20 35 39 6 # proportions will convert the table into proportions instead of counts # so will prop.table and I often use that function because I learned it first # there are many different ways to accomplish the same goal in R survival_prob &lt;- proportions(table(nest_offspring)) # How do these values compare to the calculated probability of all three chicks # surviving? What about of none surviving? # With the values calculated from the simulated data (i.e. not the parameter # value that we fixed), what is the probability that *at least* one chick # from a nest survives? 0.47 + 0.27 + 0.06 # note: numbers might be different because we didn&#39;t set.seed() ## [1] 0.8 In general with coding, you want to avoid hard coding like this where you put in fixed values. It is much better to use code that is flexible if the input data changes (e.g. we are using a randomly generated dataset, so it will change every time), or if you want to change some parameter across a lot of different parts of the code. For example, instead of repeatedly typing p=0.4, we could have created an object in our environment with the probability of success such as p.surv &lt;- 0.4 and then specifying p=p.surv throughout the script, which would make it really easy to change the simulations for a new scenario (e.g. if the guillemot chicks were given little hang-gliders, we might increase p.surv and would only have to type it out once as p.surv &lt;- 0.8. Subsetting vectors in R is a really useful tool when you want to apply a function to only part of a vector, inspect part of an object, etc. We use the square brackets [] for subsetting, and within them specify which elements to return. The elements to return can be a numeric vector (e.g. c(1,2,4)) to return the first, second, and fourth elements, or a logical vector indicating if an element should or should not be returned (e.g. c(TRUE TRUE FALSE TRUE)). If subsetting a range of consecutive elements, the : operator can also be used (e.g. 1:4 is the same as c(1,2,3,4)). # To avoid hard coding our estimated percent, we can use sum() on a subset of # the table of proportions to estimate the probability at least one survives? sum(survival_prob[2:4]) ## [1] 0.8 # what is the probability at least two survive? sum(survival_prob[3:4]) ## [1] 0.45 # side note: if you&#39;re not sure which indices to subset, it can help to look at # your object; you can easily do this in RStudio by highlighting just the bit of # code with your object name and using Ctrl + Enter (or Cmd + Enter) to print # just that object to the console, i.e. if you highlight part of a line, you will # only run the highlighted bit, not the entire line 2.2.2 Coral example With the guillemots, we were assuming that each pair had three potential offspring surviving. What happens if we vastly increase the number of draws from the binomial distribution? Take corals as an example. Some broadcast spawning corals have mass synchronized spawning events where individual corals on a reef all release bundles of sperm and eggs at the same time. The reproductive success of each individual coral can still be modeled as a binomial distribution, i.e. how many of those released sperm and eggs will actually encounter a bundle of the same species and be a successful mating attempt is a random variable drawn from a Binomial distribution with a number of draws \\(N\\) equal to the number of released sperm and eggs and a probability of success \\(p\\) for each of those. \\[ offspring \\sim Binomial(N, p)\\]. Note: the \\(N\\) for denoting the number of draws from the Binomial is distinct from the n = argument in the rbinom() function. Don’t let this confuse you! \\(N\\) equates to the size = argument, while n = is the number of observations of the random variable, i.e. the total number of individual corals on the reef. # let&#39;s assume now the coral reef has 100,00 individual corals (I have no # clue if this is an accurate number, but let&#39;s roll with it) # let&#39;s also assume each individual coral releases 10,000 sperm and eggs # and the probability for each of those resulting in a successful mating event # 0.0002 (i.e. 0.002%) gametes &lt;- rbinom(n=100000, size=10000, p=0.0002) hist(gametes, breaks = 100) # what is the probability of reproductive failure? prop.table(table(gametes)) ## gametes ## 0 1 2 3 4 5 6 7 8 9 ## 0.13561 0.27176 0.27017 0.17799 0.09230 0.03482 0.01266 0.00337 0.00114 0.00015 ## 10 11 ## 0.00002 0.00001 # what&#39;s the probability of more than 5 offspring? prop.table(table(gametes&gt;5)) ## ## FALSE TRUE ## 0.98265 0.01735 # now let&#39;s increase the number of individual corals to 1 million, and also increase # the probability of success to 0.001 gametes2 &lt;- rbinom(n=1000000, size=10000, p=0.001) hist(gametes2, breaks=100) # now what is the probability of reproductive failure? prop.table(table(gametes2==0)) ## ## FALSE TRUE ## 0.999952 0.000048 # and what is the probability of more than 15 offspring? prop.table(table(gametes2&gt;15)) ## ## FALSE TRUE ## 0.95154 0.04846 # let&#39;s visualize the data as a histogram again, but add a vertical bar using # the abline() function at 15; note there are other plotting options to make # your plots slightly more aesthetic hist(gametes2, breaks=100, border=F, # i just don&#39;t like borders on bars col=&quot;turquoise4&quot;, main=&quot;&quot;, # removes the &#39;title&#39; xlab=&quot;Number of successful mating attempts&quot;, # changes x axis label las=1 # rotates the axis labels the right way ) abline(v=15, lwd=2, # width of line col=&quot;tomato2&quot;) 2.3 Probability mass In the example above, you can see that there is very low probability of being in the tails of the distribution. So far, we have only been thinking about discrete distributions where the outcome will fall into a ‘bin’ of some kind. With the Bernoulli distribution, the outcome could only be 0 or 1. With the Binomial distribution, the outcome could only be an integer, including zero, and can only be positive. There are no numbers in between; you either make it to the ocean, or you don’t, and you never end up with 1.5 surviving offspring. Discrete distributions have a probability mass function, which you can think of as the amount of probability associated with each bar in the histogram (including potential ‘bars’ that are never observed and their probability mass is 0). Discrete vs continuous data by Alison Horst 2.4 The Normal (Gaussian) distribution In contrast to discrete distributions, continuous distributions can return any number value (within some constraints, which we will get to later in the semester when we talk about generalized linear models or GLMs). For now, we will just focus on the normal distribution which is one that is likely most familiar. The normal distribution is continuous and can return any value from \\(-\\infty\\) to \\(+\\infty\\). There are two parameters that govern the normal distribution: 1) the mean of the distribution, \\(\\mu\\) and the variance, \\(\\sigma^2\\). If we have a response variable \\(y\\) that is a random variable drawn from the normal distribution, we would write this out as: \\[y \\sim Normal(\\mu, \\sigma^2)\\] The mean (\\(\\mu\\)) is the central tendency of the distribution; you could also think of it as the expected value. The variance (\\(\\sigma\\)) is a measure of spread, i.e. how tightly are values clustered to that central tendency versus more spread out. 2.4.1 Guillemot weight Apparently, adult Common Guillemots weigh 900-1100 grams (about 2-2.5 lbs). Because guillemot weights can be any number (e.g. 929.374 grams), we could assume that our response variable (weight) is a random variable drawn from a normal distribution. Ballpark, let’s say that the mean is 980, and the variance is 400. In R, instead of using the variance directly, the function rnorm() takes the standard deviation (\\(\\sigma\\)) as input. The standard deviation is simply the square root of the variance. So we can describe guillemot weight mathematically as: \\[ weight \\sim Normal(\\mu = 980, \\sigma=20)\\] Using the function rnorm, we will simulate a population of 120 adult guillemots and their weights. adult_weights &lt;- rnorm(n = 120, # population size mean = 980, # mu, population mean sd = 20 #standard deviation, square root of the variance ) # plot a histogram of our sample hist(adult_weights, breaks = 20) # add a vertical line where we set the mean to # note: if running these in an R Markdown file, you have to run both lines # at the same time or you will get an error that plot.new has not been called abline(v = 980, lwd=2, col=&quot;red&quot;) # based on the sample, what is the probability of an adult guillemot weighing # more than 980 grams? proportions(table(adult_weights&gt;980)) ## ## FALSE TRUE ## 0.5166667 0.4833333 # what is the probability of an adult weighing exactly 990 grams? (not 990.1, 990.2, etc.) # we could do this empirically, i.e. how many of our 120 adults were exactly 990 grams? proportions(table(adult_weights==990)) ## ## FALSE ## 1 # but this does not make any sense, because we are dealing with continuous data, so why # would we treat it like it is in discrete chunks? 2.5 Probability density Because we are dealing with a continuous distribution, there are no finite points along the distribution that can have a probability associated with them. Instead, we need to think back to calculus and consider the probability of any given value as an integral. I will derive Imagine that you have a curve, and the area under the curve represents a probability, so it must sum to 1 (i.e. all possible outcomes must be represented). Picture a set of discrete bars underneath that distribution, each representing some portion of the total probability, like we did with the discrete distributions. We will use the function dnorm() for this, which is the density (d for density) of a normal distribution at a vector of values, rather than random draws (r for random). To create our vector of values, we will use the seq() function, which generates a sequence of numbers. barplot(dnorm(seq(from=-3, to=3, by=0.5))) Now imagine that we make each bin smaller, and smaller, and smaller by changing the increments in our sequence (i.e. we make more bins). barplot(dnorm(seq(from=-3, to=3, by=0.1))) barplot(dnorm(seq(from=-3, to=3, by=0.05))) barplot(dnorm(seq(from=-3, to=3, by=0.01))) As the size of our bins approaches zero, the number of bins approaches infinity. This is essentially what we are doing with an integral. With a continuous distribution, we now have probability density functions, rather than probability mass. To get back to our original question of the probability of an adult guillemot weighing exactly 990 grams (still a silly question), we can use the dnorm() function to get the probability density at that value. dnorm(x=990, mean=980, sd=20) ## [1] 0.01760327 What we are likely more interested in, however, is the probability of an adult weighing at least 990 grams. For that, we need the pnorm() function which returns the cumulative density of the normal distribution at a given quantile (for now, think of the quantile as just the cutoff point in adult weights that we are interested in). # what is the probability of observing an adult guillemot that weighs at least 990 grams? pnorm(q = 990, mean = 980, sd = 20, lower.tail=F # we need to specify that we are in the upper tail ) ## [1] 0.3085375 # what about at least 1000 grams? pnorm(q = 1000, mean = 980, sd = 20, lower.tail = F) ## [1] 0.1586553 # what about at least 1020 grams? pnorm(q = 1020, mean = 980, sd = 20, lower.tail = F) ## [1] 0.02275013 As we get into the tails of the distribution, the probability of observing an adult guillemot that is at least that heavy, or heavier, gets increasingly less likely. This brings us to the definition of the p-value: the probability of observing your data, or data more extreme, if the null is true. In our case, we are treating the null hypothesis as being that guillemot weights are drawn from a normal distribution with a mean of 980 and standard deviation of 20. What if instead we our null hypothesis was that the mean was still 980, but that there was a lot more variability in nature, and the standard deviation was 30? The probability of an adult guillemot weighing that much is now much higher (9.1%, as opposed to 0.02%) because there is more spread around the central tendency. pnorm(q=1020, mean=980, sd=30, lower.tail=F) ## [1] 0.09121122 2.6 The Standard Normal Distribution In the real world, we typically do not know what the parameters are underlying our distribution (i.e. we would have know way of knowing what the mean and standard deviation of guillemot weight are). We also often work with data that are normally distributed, but could not have negative values. Remember that the support of the normal distribution is all numbers from \\(-\\infty\\) to \\(+\\infty\\). Adult guillemots, however, cannot have negative weight. Thus, we first need to centralize and standardize the data so that we know what the mean and standard deviation are. Centralizing data means we subtract the mean value of our sample from every single observation, such that we are left with a mean value of 0. Standardizing data is a way of transforming it so that variables are more comparable across different data sources, and also so that we can easily compare standard deviations. To standardize data, we convert it to Z-scores. For each observation \\(x\\), we subtract the mean \\(\\bar{x}\\) and then divide that value by the standard deviation \\(\\sigma\\). The equations to do this for a sample are below; \\(SS\\) stands for the sum of squared residuals. Note that we divide the sum of squares by n-1, rather than n, because we are dealing with a sample rather than population. The degrees of freedom is the number of observations we have, but we lost one degree of freedom when we calculated the sample mean \\(\\bar{x}\\). \\[Z = (x -\\bar{x})/\\sigma\\] \\[\\sigma = \\sqrt{\\sigma^2}\\] \\[\\sigma^2 = \\frac{SS}{(n-1)}\\] \\[SS = \\sum_{i=1}^{n}(x_i - \\bar{x})^2\\] 2.6.1 Functions To standardize our data we need to center it (subtract the mean) and standardize (Z transform) the data. We will create functions in R to do this for us quickly. There are many inbuilt functions in R, such as the ones we have been using already (e.g. rnorm, hist, sum, etc.) and you can also install packages that contain functions which other people have written. You can also write your own custom functions. This is extremely useful when you want to repeat an operation many times without using up many lines of code (the more lines of code, the more likely you are to end up with an error, or something difficult to fix later on). First, we will create a function that calculates just the sum of the squared residuals. To create a function, we need to name it just like an object, but pass to it a function() containing arguments that the function accepts. In this case, it is a function that will take a vector x. Inside the {} brackets we place the instructions for what to do with x and what to return from the function. Only the object that is specified will be returned (i.e. xbar, res, and squares will not be saved to your environment, only ss). sum_squares &lt;- function(x){ xbar &lt;- mean(x) # mean of the sample res &lt;- x - xbar # residuals squares &lt;- res^2 # squared residuals ss &lt;- sum(squares) # sum of the squared residuals return(ss) # what should the function return } We can also nest functions inside other functions, which can make it a lot easier to keep your code neat and tidy. Below is a function to standardize data which uses the other function we just created. standardize &lt;- function(x){ xbar &lt;- mean(x) n &lt;- length(x) ss &lt;- sum_squares(x) # note we call our function from earlier s2 &lt;- ss/(n - 1) # variance = sum of squares divided by n - 1 s &lt;- sqrt(s2) # calculate standard deviation from variance z &lt;- (x - xbar)/s return(z) } # use this function to standardize adult guillemot weights that we generated # earlier and plot the output. how does it differ from the original? std_weights &lt;- standardize(adult_weights) hist(std_weights) # Note that we now have negative weights, however, the interpretation is simply # that these individuals are below the mean, and positive values are above it One of the nice things about the standard normal distribution is that we know that the mean is 0 and the standard deviation is 1. This means that now, when we ask about the probability of observing an adult guillemot with a standardized weight of some value or higher and our null is 0, we are asking if that individual significantly differs from the sample mean. We also know the standard deviations, and instead of having to calculate the integral at different cutoffs, this has already been done. Prior to advances in modern computing, this was done manually, and there were huge lookup tables at the back up statistics textbooks that would tell you the p-value associated with different z-scores. Thankfully, now we have R. "],["404.html", "Page not found", " Page not found The page you requested cannot be found (perhaps it was moved or renamed). You may want to try searching to find the page's new location, or use the table of contents to find the page you are looking for. "]]

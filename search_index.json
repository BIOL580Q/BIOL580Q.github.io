[["index.html", "BIOL 480/580Q: Ecological Modeling (Fall 2024) 1 Introduction 1.1 About this ‘book’ 1.2 Why this course? 1.3 Getting comfortable with R and RStudio 1.4 How to approach this course", " BIOL 480/580Q: Ecological Modeling (Fall 2024) Eliza Grames 1 Introduction 1.1 About this ‘book’ These are my notes to accompany lectures for BIOL 480Q/580Q: Ecological Modeling (Fall 2024). They are not meant to be a textbook by any means, but rather a pretty casual introduction to statistics and modeling in R relevant to ecologists. Because the course is cross-listed as a graduate and undergraduate course, I assume many folks are coming to it with minimal background in probability, statistics, or coding, so this should hopefully be a pretty gentle introduction. Also, I am writing it up as we go along in the course this semester, so more sections will be added as we go along. I will do my best to tell you which section(s) we will cover each week but we may spill over if we get through something quickly, or spend more time on some concepts than I anticipate. 1.2 Why this course? Just before I started graduate school nearly a decade ago, a survey found that most early career ecologists lacked adequate quantitative training and 75% were not satisfied with their own understanding of ecological modeling. Ecology has always been heavily rooted in mathematics, and has become increasingly quantitative in recent decades, but graduate training has lagged behind that trend. I was lucky to have taken an Ecological Modeling course taught by two brilliant quantitative ecologists (and excellent human beings) - Morgan Tingley and Robi Bagchi - during my PhD at the University of Connecticut. This course is inspired by theirs, and similarly aims to train the next generation of quantitative ecologists. Figure from Barraquand et al. (2014) showing the relationship between early career ecologists’ satisfaction with their understanding of mathematical models and how involved they are in ecological modeling. One of the reasons I teach this course is because I think it is really important for students to really understand their data and how to properly analyze it to address their hypotheses. Too often, the way statistics is taught (including how I was first introduced to it as an undergrad) is very rigid, with strict rules to follow, assumptions to be met, and prescribed ways of doing things. Many of us have come across guidelines like these charts for how to pick the right statistical test for your data. I also think that this approach to learning statistics instills a wariness about doing analyses ‘wrong’, which can turn many students away from statistics and modeling. I have absolutely no data to back this up, but I suspect the rigid way in which statistics is taught is one reason for a gender gap in quantitative ecology. Gender stereotypes can cause girls to internalize that they are ‘bad at math’ at an early age, which combined with a fear of not getting the ‘rules’ of statistics right, could be one reason why only 4% of early career women in ecology indicate being very involved in ecological modeling compared to 10% of men. Gender differences in how PhD students and postdocs in North America rate their involvement in ecological modeling (5 being high, 1 being low). Figure derived from supplemental materials in Barraquand et al. (2014). Gender differences in how PhD students and postdocs in North America rate their level of comfort in using equations for ecological modeling (5 being high, 1 being low). Figure derived from supplemental materials in Barraquand et al. (2014). It is also really important for students to develop quantitative skills for their future employment prospects. For graduate students aiming for academic careers, quantitative skills can help them secure permanent positions. 40% of faculty job listings in ecology and related fields require some level of quantitative skills, and 21% require “strong” quantitative skills. Many industry and government research positions also require quantitative skills, including data analysis, visualization, and modeling. Perhaps most importantly, learning how to code and build models opens up a whole new world of possibilities for asking interesting questions. More often than not, we address questions using the tools we have at our disposal (i.e. if you’ve got a hammer, everything looks like a nail). The type of statistical tests that students often learn are inadequate to deal with many of the questions and hypotheses in ecology and related fields. Instead of being constrained by what statistical tests will be possible with the data or cramming the data into a test that does not really fit, students can learn to build models that truly address their research questions, opening up the possibility for more novel hypotheses. 1.3 Getting comfortable with R and RStudio We will be using almost exclusively R, accessed through RStudio, for this course. R is a programming language, so you will not access it directly. Instead, we will use RStudio, which is a graphical user interface (GUI) that makes it a lot easier to use R for your analyses. Install R. Follow the instructions for your operating system here: https://cran.r-project.org/ Install RStudio. Follow the instructions for your operating system here: https://posit.co/download/rstudio-desktop/ Sometimes, your computer will add a shortcut icon to access R directly which will pull up an older looking GUI. Do not use it. Always use RStudio to interact with R (…unless you’re running shell scripts or some other niche purposes, but if you’re doing that, you probably aren’t reading instructions on how to access R). The logo on the right is RStudio, which is the one you want. When you open RStudio for the first time, it should look something like this: In the upper left is the Source pane. This is where you will open scripts, edit code, run analyses, type notes, etc. To use a cooking analogy, the Source pane is your recipe book. You can always come back to it, make little changes, or leave notes about what to fix for next time. But it is not the actual meal, just instructions for making a meal. In the bottom left is the Console. The Console is the process of cooking. It can be done following the recipe by running chunks of code from the Source, or you can wing it. If you decide to ‘cook’ on the fly by running code directly from the Console, you will save no notes on what on earth you put into the meal, you cannot go back and fix things easily, and you cannot share your recipe with anyone else. You also might not remember the order in which you did things. So if what you did worked and your ‘meal’ comes out great, you will not know what you did to make that happen so it is not reproducible. The only real use for the console is for code that you only want to run once, e.g. foundational things like installing packages; to really overcook this analogy (ha), you only install a kitchen counter once, not at the start of every recipe. More on packages later. In the upper right is the Environment (along with a few other tabs that you’ll use far less). This is the current version of your meal, i.e. if you’re making a stew, this is everything that is currently in the pot or mise en place and ready to go. It is objects (think of these as ingredients) that you have created with your cooking and are now available to either eat or combine with other objects (i.e. ingredients). In the bottom right are your Files, Plots, and Help - all of which you will use frequently (and other tabs you will use far less often). Think of these as things that are available to you, but external to your current kitchen counter and stew. The Files tab will let you browse files on your computer; think of it is your kitchen where you can get more resources for your meal if you are in your current working directory (more on directories later), or your house if you browse for other files on your computer. Your Plots tab is the equivalent of taking a picture of the current version of your meal, either to be able to visualize what you’ve currently got, or to share with others. The Help tab is exactly what it sounds like - the person you call to ask how to use the stand mixer (i.e. the function) and what to put into it (i.e. the arguments passed to the function) to make the perfect waffle. It’s up to you to know that you want to use the stand mixer to make waffles in the first place. 1.3.1 Some terms and definitions Since that analogy is now burnt to a crisp, let’s unpack some of the extra terms in there and what they are because you will use them frequently in this course. 1.3.1.1 Packages Packages are a collection of functions designed to work together to accomplish some specific outcome. Many packages are hosted on CRAN, however, you can also find R packages on repositories like GitHub. One way to find packages is through CRAN Task Views (e.g. these are all packages associated with meta-analysis https://cran.r-project.org/web/views/MetaAnalysis.html), but more often you’ll just Google what you want to do and find a package that way. To install a package from CRAN, you can use the install.packages() function with the name of the package inside the open parentheses in quotes (e.g. install.packages(\"lme4\")) to install the lme4 package which is useful for linear models. To install packages from other sources, follow the package developer’s instructions. 1.3.1.2 Objects Objects are what are in your environment. They can take lots of different forms, have different classes, etc. Most objects are created by using the assignment operator &lt;- to pass the output of some code to a named object. We will talk about different types of objects more during the semester as we encounter them. 1.3.1.3 Working directory Your Files tab lets you see two things: files, and directories. Directories are the organizational structure for how you store data on your computer; you can think of them as folders for the most part, though folders are GUI ways to visualize directories and directories have a clear nested structure. You working directory is very important when coding. File paths are relative to your current working directory, so when you read files in you must know both what your current working directory is; most of the time if you get an error reading in a file, it is because the path to the file is incorrect. In the cooking analogy, you are cooking on the kitchen countertop, which is nested within the kitchen, which is nested within your house. You could move directories within the kitchen, such as moving to the sink, or you could move up several levels to go to the living room. If you try to call a file that is not in your current working directory, you will get an error. For example, if you are in the living room and tell R to pick up your cutting board, it will say it does not exist. Relative file paths are extremely useful in coding. Relative file paths begin in your current working directory. To load a file from your current directory, begin the file path with ./. To load a file from the working directory above you (i.e. if you’re working at the kitchen counter, the kitchen is the next hierarchical level above you), use ../. One way to remember the difference between one dot and two is that if the dots represent your feet, one shows where you are standing - but two means you’ve hopped somewhere new. Since directories are nested within each other, you can also combine these into longer relative paths. So, for example, if I am in the living room but I want R to go into the kitchen, then into the cabinet, and take out a cutting board, the path would be something like: ../kitchen/cabinet/cutting_board.txt. Setting your working directory. There are two ways to set your Working Directory. One is the click-and-point way, where you can go to Session &gt; Set Working Directory &gt; Choose Location which will open up your normal file browser application and you can navigate around to find where you want to set as your current working directory. This is easier when you’re starting out and getting used to directory structure and how you have your files organized, but is not reproducible so can cause headaches if you think you’re in a different directory than you are later on when running a script. A better option is to use the function setwd() with an absolute path to a directory. An absolute path is one that starts at your home directory (e.g. on Linux or Mac, ~/, on Windows typically something like C:/). For example, I might run something like setwd(\"~/Desktop/480Q\") at the start of my script, and then use relative paths throughout once I am in the directory where I have my files for analysis and where I want to save my output. 1.3.1.4 Functions Functions take input as arguments, and return output. To see what arguments can be passed to a function, and also what its output will be, you can use the Help tab to search for a function. Or, much more quickly, use a ? followed by the function name if it is in a package that is currently loaded (e.g. ?rbinom). If it is in a package that is not currently loaded, use ?? instead (e.g. ??glmer). 1.3.2 Customizing RStudio There are two modifications many people will want to make to RStudio: To change the theme, go to Tools &gt; Global Options. For example, you may prefer a dark theme if you’re coding frequently. Rearrange the panes. There is a window-like icon to the left of ‘Addins’ in the tool bar; select the drop down and you can customize which panes are in which corners. For example, I prefer to move my Console to the right so I can see my code in parallel and make the Environment tab really small because I do not need to check it frequently. 1.4 How to approach this course The goal of this course is to get you to think quantitatively i.e. recognizing that data are generated by deterministic and stochastic processes, both of which are defined by you (or your understanding of the biology of the system that you are modeling or making inference on). No one can really tell you how to model or analyze your data - there is no lookup table for ecological modeling. After this course, you should know enough to 1) be dangerous on Google / StackOverflow to find analytical / modeling approaches that match your questions and data, and 2) write or adapt code to implement those models and not be reliant on a GUI. The goal is not complete mastery of statistics, modeling, or data analysis. The goal of this course is to get you closer to becoming dangerous. 1. It is not about content mastery. This course likely requires a different mindset from what you have encountered previously, primarily because it is not about content mastery. To be a specialist in any field, including biology, there is a lot of content that you need to have under your belt, which is why many undergraduate (and graduate) courses emphasize understanding principles and knowing certain key information (e.g. you need to know population dynamics, cell division, anatomy and physiology, evolutionary principles, etc.). In contrast, this course is much more about building your toolset and learning techniques, then recombining them in new ways when faced with different modeling problems. In many ways, learning how to analyze your data, how to code, how to specify models, etc. is like learning how to cook. Think of content mastery as foraging - if you are in the woods collecting mushrooms, you must know which ones are edible and which are poisonous and how to tell the difference. Knowing what you can and cannot eat is essential background information. Modeling, on the other hand, is more like cooking - what are you going to do with what you’ve gathered? 2. It is better to know of many methods than to know a few in-depth. We have 28 hours together this semester. With that time, I could teach you one specific model each week and we could go very in-depth to know all the details, assumptions, modifications, etc. To put it in a cooking context, I could teach you one recipe each week and by the end of the course you would know how to make 14 of my favorite dishes. Or, instead, you could be exposed to many different types of food that you might like to make depending on what you’re in the mood for, covering a variety of cooking styles, techniques, and cuisines. Knowing that these dishes exist and having a general sense of what you want to make gives you many more options than just knowing how I make guacamole. 3. Not all methods are suitable for your research questions. Even knowing that there are many dishes you could make, not all of them will suit you. Some weeks the dishes won’t work for your dietary needs (e.g. maybe you never have nested data) or you will wonder why anyone would ever want to cook French food. Then, we will get to Mexican food the next week (er, I mean structural equation modeling) and it will be exactly what you were looking for. Even on weeks that are not currently relevant to your research, it is still good to know that they exist and you may encounter those methods in other ways and be glad you have a basic understanding (e.g. as a peer reviewer). 4. You can always look up how to implement a method. If you know a dish exists but you are not entirely sure how to make it, you can look up a recipe and follow along. The same is true of models. Some recipes are not exactly what you want, but if you know what you’re looking for, eventually you’ll find a good example, guide, or tutorial. And sometimes, you may go to a restaurant (i.e. a journal article) and you really like it and try to recreate it! 5. Practicing is important. The first time you try a new recipe, it may not go well, but you’ll get better with practice. And at least you tried! With coding, you will need to keep at it and you will learn new tricks the more you practice. 6. Selecting methods a priori is better than post-hoc. Sometimes, you have been handed ingredients and are trying to find a recipe that will work with what you have. For example, in a data context, your PI may give you a dataset, you may be working with historical data, or you collected the data before having a plan for analysis. In an ideal world, you know what you are going to cook in advance so you have all the ingredients that you need on hand. Figuring out which models are possible post-hoc. Planning your analyses before collecting data, "],["probability.html", "2 Probability 2.1 Bernoulli distribution 2.2 Binomial distribution 2.3 Probability mass 2.4 The Normal (Gaussian) distribution 2.5 Probability density 2.6 The Standard Normal Distribution", " 2 Probability 2.1 Bernoulli distribution If you flip a coin, what is the probability that it lands up heads? There is a 0.50 probability, or 50% chance, of flipping heads. There is also a 0.5 probability that the outcome is tails. These two probabilities have to sum to 1 because there is no chance of any other outcome. Put another way, if \\(p_h = 0.5\\), then \\(p_t = 1 - p_h = 0.5\\), where \\(p_h\\) is probability of heads and \\(p_t\\) is probability of tails. Now, instead of thinking about flipping a coin as a 50/50 shot at heads or tails, think about it only in terms of succeeding at flipping heads. Each successful attempt you get a ‘1’ for a success, and each time the coin is not heads, we call it a ‘0’ for failure. Put another way, we ask the question ‘Did we flip heads?’ and if the answer is TRUE, and we use a 1 to denote that. This is actually how R encodes TRUE (T = 1) and FALSE (F = 0). We can use R to simulate a coin flip using the function rbinom(). # to leave notes to yourself in R scripts, start with a # ?rbinom # look up the Help file to find out about the arguments to the function # rbinom wants: # n, the number of observations / replicates # size, the number of draws # p the probability of success # to simulate a single coin flip, we want to take one replicate (n=1) of # one draw (size=1) from a binomial distribution with a probability of # success (heads) of 0.5 rbinom(n=1, size=1, prob=0.5) ## [1] 0 # we could keep running this line over and over # or we could increase n, the number of replicates # this gives us a vector of 100 observations of a coin flip rbinom(n=100, size=1, p=c(0.5)) ## [1] 0 0 1 1 1 1 1 0 0 1 0 0 0 0 0 0 1 0 0 0 1 1 1 0 0 1 1 0 0 1 0 1 1 1 1 1 1 ## [38] 0 1 0 0 0 0 0 0 0 0 1 0 1 0 0 1 1 1 0 0 1 1 1 1 1 1 0 1 1 0 1 0 1 0 0 0 1 ## [75] 1 0 0 0 0 0 1 1 1 1 1 1 0 1 0 1 1 0 1 0 0 1 0 1 0 0 # to visualize the outcome, we can create a histogram plotting the results # using the function hist() hist(rbinom(n=100, size=1, p=0.5)) # because the function is drawing from a probability distribution, outcomes will # be a little different every single time. If you need them to be the same, you # can set the seed with set.seed(42), or any other number The outcome we are observing, successfully flipping heads, is a random variable. A random variable is one that is drawn from a probability distribution (not in the colloquial sense of the word random). Our observed response (heads = TRUE) is drawn from a Bernoulli distribution with a probability of success of 0.5. We can write this mathematically as: \\[heads \\sim Bernoulli(p=0.5)\\] The Bernoulli distribution is a probability distribution that only yields success (1) or failure (0) based on the probability of success \\(p\\). The probability can vary depending on the process you are modeling, but the outcome will always be 0 or 1. As an ecological example, let’s take guillemot chicks. Guillemots are seabirds that nest on steep cliffsides to escape predators. They spend most of their life on the ocean, however, and as such are very awkward on land and also not great at flying. To get from the cliffside nest to the ocean for the first time, guillemot chicks have to jump and then glide/fly. If they do not make it all the way, they can attempt to run, but are vulnerable to predation. The only option for chicks is to survive or fail - making it 70% of the way to the ocean will result in failure (and, in this case, death). # let&#39;s assume chicks have a 40% chance of making it to the ocean, and there are # 100 chicks jumping from the cliff hoping to make it to the ocean so we have # 100 observations, each with one draw from the Bernoulli distribution rbinom(n = 100, size=1, p=0.4) ## [1] 0 1 0 0 0 1 1 1 0 0 0 0 0 0 0 0 1 0 0 0 0 1 0 1 0 1 0 0 0 0 0 1 0 0 0 0 0 ## [38] 1 0 0 0 0 1 0 0 0 0 1 0 0 1 1 1 0 0 0 1 1 0 1 0 0 1 0 0 0 1 0 0 1 0 0 0 0 ## [75] 1 0 1 0 1 1 0 0 1 0 1 0 0 0 1 0 1 1 0 1 0 0 0 0 1 0 # so far, we have just been running functions and the output is shown in the console # but we don&#39;t have anything in our environment # can save to an object with the &lt;- assignment operator survival &lt;- rbinom(n=100, size=1, p=0.4) # we can then do things with that saved object, like plot it hist(survival) 2.2 Binomial distribution 2.2.1 Guillemot example Guillemots typically only lay 1-2 eggs, but for the sake of our example code, let’s pretend that each pair has 3 chicks that are attempting to make it to the ocean. Each individual chick’s attempt could be modeled with a Bernoulli distribution, but if we consider it from the perspective of the breeding pair, the total number of surviving offspring is what matters. We can think of the number of chicks from each nest that make it to the ocean as the sum of their individual attempts for a single nest. This is the Binomial distribution, which is a more generalized version of the Bernoulli distribution (more accurately, the Bernoulli distribution is a special case of the Binomial distribution with one draw) in which each observation has multiple draws. # for one nest with 3 chicks and a probability of success of 0.4 rbinom(n=1, size=3, p=0.4) ## [1] 0 # we could also think about a colony with let&#39;s say 100 nests # how many chicks from each nest will make it? nest_offspring &lt;- rbinom(n=100, size=3, p=0.4) hist(nest_offspring) nest_offspring ## [1] 2 2 1 2 2 2 0 1 1 2 0 2 0 2 1 0 1 2 1 1 3 0 3 0 1 1 0 1 2 1 0 0 1 3 2 2 2 ## [38] 1 0 1 0 0 3 0 0 0 1 2 2 2 1 0 0 1 0 2 1 1 1 1 3 1 2 3 3 1 1 2 2 1 1 3 1 2 ## [75] 0 0 2 1 0 1 1 2 2 1 0 2 0 0 0 1 2 1 1 1 0 2 0 1 2 0 # what is the probability that all three chicks from a nest make it? # because we know what we fixed p to, we could multiply the probabilities 0.4^3 ## [1] 0.064 # what is the probability that none make it? (1 - 0.4)^3 ## [1] 0.216 We are able to calculate what the probability of these outcomes are because we fixed the probability of success at 0.4 and then simulated outcomes. In the real world, we rarely know what the probability of success truly is, and instead we are working backwards from our observed outcomes (i.e. our data) to estimate the probability of survival. One of the beautiful things about working with simulated data is that we know the input value, so we can test assumptions, see if we recapture our input, and play around with different model structures and know that anything that is unexpected is most likely a problem with our code or model, not the data. With discrete distributions like the Binomial, we can convert the frequency of outcomes to a proportion of the total as an estimate of probability. Dividing the frequencies by the total sample size makes it so that the total probability sums to 1. # table is a function that tallies up all the items in a vector # a vector is a type of object that is one dimension, i.e. nest_offspring is a # vector with length = 100 table(nest_offspring) ## nest_offspring ## 0 1 2 3 ## 28 36 28 8 # proportions will convert the table into proportions instead of counts # so will prop.table and I often use that function because I learned it first # there are many different ways to accomplish the same goal in R survival_prob &lt;- proportions(table(nest_offspring)) # How do these values compare to the calculated probability of all three chicks # surviving? What about of none surviving? # With the values calculated from the simulated data (i.e. not the parameter # value that we fixed), what is the probability that *at least* one chick # from a nest survives? 0.47 + 0.27 + 0.06 # note: numbers might be different because we didn&#39;t set.seed() ## [1] 0.8 In general with coding, you want to avoid hard coding like this where you put in fixed values. It is much better to use code that is flexible if the input data changes (e.g. we are using a randomly generated dataset, so it will change every time), or if you want to change some parameter across a lot of different parts of the code. For example, instead of repeatedly typing p=0.4, we could have created an object in our environment with the probability of success such as p.surv &lt;- 0.4 and then specifying p=p.surv throughout the script, which would make it really easy to change the simulations for a new scenario (e.g. if the guillemot chicks were given little hang-gliders, we might increase p.surv and would only have to type it out once as p.surv &lt;- 0.8. Subsetting vectors in R is a really useful tool when you want to apply a function to only part of a vector, inspect part of an object, etc. We use the square brackets [] for subsetting, and within them specify which elements to return. The elements to return can be a numeric vector (e.g. c(1,2,4)) to return the first, second, and fourth elements, or a logical vector indicating if an element should or should not be returned (e.g. c(TRUE TRUE FALSE TRUE)). If subsetting a range of consecutive elements, the : operator can also be used (e.g. 1:4 is the same as c(1,2,3,4)). # To avoid hard coding our estimated percent, we can use sum() on a subset of # the table of proportions to estimate the probability at least one survives? sum(survival_prob[2:4]) ## [1] 0.72 # what is the probability at least two survive? sum(survival_prob[3:4]) ## [1] 0.36 # side note: if you&#39;re not sure which indices to subset, it can help to look at # your object; you can easily do this in RStudio by highlighting just the bit of # code with your object name and using Ctrl + Enter (or Cmd + Enter) to print # just that object to the console, i.e. if you highlight part of a line, you will # only run the highlighted bit, not the entire line 2.2.2 Coral example With the guillemots, we were assuming that each pair had three potential offspring surviving. What happens if we vastly increase the number of draws from the binomial distribution? Take corals as an example. Some broadcast spawning corals have mass synchronized spawning events where individual corals on a reef all release bundles of sperm and eggs at the same time. The reproductive success of each individual coral can still be modeled as a binomial distribution, i.e. how many of those released sperm and eggs will actually encounter a bundle of the same species and be a successful mating attempt is a random variable drawn from a Binomial distribution with a number of draws \\(N\\) equal to the number of released sperm and eggs and a probability of success \\(p\\) for each of those. \\[ offspring \\sim Binomial(N, p)\\]. Note: the \\(N\\) for denoting the number of draws from the Binomial is distinct from the n = argument in the rbinom() function. Don’t let this confuse you! \\(N\\) equates to the size = argument, while n = is the number of observations of the random variable, i.e. the total number of individual corals on the reef. # let&#39;s assume now the coral reef has 100,00 individual corals (I have no # clue if this is an accurate number, but let&#39;s roll with it) # let&#39;s also assume each individual coral releases 10,000 sperm and eggs # and the probability for each of those resulting in a successful mating event # 0.0002 (i.e. 0.002%) gametes &lt;- rbinom(n=100000, size=10000, p=0.0002) hist(gametes, breaks = 100) # what is the probability of reproductive failure? prop.table(table(gametes)) ## gametes ## 0 1 2 3 4 5 6 7 8 9 ## 0.13461 0.27222 0.27040 0.18081 0.08890 0.03632 0.01254 0.00298 0.00086 0.00029 ## 10 11 ## 0.00005 0.00002 # what&#39;s the probability of more than 5 offspring? prop.table(table(gametes&gt;5)) ## ## FALSE TRUE ## 0.98326 0.01674 # now let&#39;s increase the number of individual corals to 1 million, and also increase # the probability of success to 0.001 gametes2 &lt;- rbinom(n=1000000, size=10000, p=0.001) hist(gametes2, breaks=100) # now what is the probability of reproductive failure? prop.table(table(gametes2==0)) ## ## FALSE TRUE ## 0.999953 0.000047 # and what is the probability of more than 15 offspring? prop.table(table(gametes2&gt;15)) ## ## FALSE TRUE ## 0.95138 0.04862 # let&#39;s visualize the data as a histogram again, but add a vertical bar using # the abline() function at 15; note there are other plotting options to make # your plots slightly more aesthetic hist(gametes2, breaks=100, border=F, # i just don&#39;t like borders on bars col=&quot;turquoise4&quot;, main=&quot;&quot;, # removes the &#39;title&#39; xlab=&quot;Number of successful mating attempts&quot;, # changes x axis label las=1 # rotates the axis labels the right way ) abline(v=15, lwd=2, # width of line col=&quot;tomato2&quot;) 2.3 Probability mass In the example above, you can see that there is very low probability of being in the tails of the distribution. So far, we have only been thinking about discrete distributions where the outcome will fall into a ‘bin’ of some kind. With the Bernoulli distribution, the outcome could only be 0 or 1. With the Binomial distribution, the outcome could only be an integer, including zero, and can only be positive. There are no numbers in between; you either make it to the ocean, or you don’t, and you never end up with 1.5 surviving offspring. Discrete distributions have a probability mass function, which you can think of as the amount of probability associated with each bar in the histogram (including potential ‘bars’ that are never observed and their probability mass is 0). Discrete vs continuous data by Alison Horst 2.4 The Normal (Gaussian) distribution In contrast to discrete distributions, continuous distributions can return any number value (within some constraints, which we will get to later in the semester when we talk about generalized linear models or GLMs). For now, we will just focus on the normal distribution which is one that is likely most familiar. The normal distribution is continuous and can return any value from \\(-\\infty\\) to \\(+\\infty\\). There are two parameters that govern the normal distribution: 1) the mean of the distribution, \\(\\mu\\) and the variance, \\(\\sigma^2\\). If we have a response variable \\(y\\) that is a random variable drawn from the normal distribution, we would write this out as: \\[y \\sim Normal(\\mu, \\sigma^2)\\] The mean (\\(\\mu\\)) is the central tendency of the distribution; you could also think of it as the expected value. The variance (\\(\\sigma\\)) is a measure of spread, i.e. how tightly are values clustered to that central tendency versus more spread out. 2.4.1 Guillemot weight Apparently, adult Common Guillemots weigh 900-1100 grams (about 2-2.5 lbs). Because guillemot weights can be any number (e.g. 929.374 grams), we could assume that our response variable (weight) is a random variable drawn from a normal distribution. Ballpark, let’s say that the mean is 980, and the variance is 400. In R, instead of using the variance directly, the function rnorm() takes the standard deviation (\\(\\sigma\\)) as input. The standard deviation is simply the square root of the variance. So we can describe guillemot weight mathematically as: \\[ weight \\sim Normal(\\mu = 980, \\sigma=20)\\] Using the function rnorm, we will simulate a population of 120 adult guillemots and their weights. adult_weights &lt;- rnorm(n = 120, # population size mean = 980, # mu, population mean sd = 20 #standard deviation, square root of the variance ) # plot a histogram of our sample hist(adult_weights, breaks = 20) # add a vertical line where we set the mean to # note: if running these in an R Markdown file, you have to run both lines # at the same time or you will get an error that plot.new has not been called abline(v = 980, lwd=2, col=&quot;red&quot;) # based on the sample, what is the probability of an adult guillemot weighing # more than 980 grams? proportions(table(adult_weights&gt;980)) ## ## FALSE TRUE ## 0.4833333 0.5166667 # what is the probability of an adult weighing exactly 990 grams? (not 990.1, 990.2, etc.) # we could do this empirically, i.e. how many of our 120 adults were exactly 990 grams? proportions(table(adult_weights==990)) ## ## FALSE ## 1 # but this does not make any sense, because we are dealing with continuous data, so why # would we treat it like it is in discrete chunks? 2.5 Probability density Because we are dealing with a continuous distribution, there are no finite points along the distribution that can have a probability associated with them. Instead, we need to think back to calculus and consider the probability of any given value as an integral. Imagine that you have a curve, and the area under the curve represents a probability, so it must sum to 1 (i.e. all possible outcomes must be represented). Picture a set of discrete bars underneath that distribution, each representing some portion of the total probability, like we did with the discrete distributions. We will use the function dnorm() for this, which is the density (d for density) of a normal distribution at a vector of values, rather than random draws (r for random). To create our vector of values, we will use the seq() function, which generates a sequence of numbers. barplot(dnorm(seq(from=-3, to=3, by=0.5))) Now imagine that we make each bin smaller, and smaller, and smaller by changing the increments in our sequence (i.e. we make more bins). barplot(dnorm(seq(from=-3, to=3, by=0.1))) barplot(dnorm(seq(from=-3, to=3, by=0.05))) barplot(dnorm(seq(from=-3, to=3, by=0.01))) As the size of our bins approaches zero, the number of bins approaches infinity. This is essentially what we are doing with an integral. With a continuous distribution, we now have probability density functions, rather than probability mass. To get back to our original question of the probability of an adult guillemot weighing exactly 990 grams (still a silly question), we can use the dnorm() function to get the probability density at that value. dnorm(x=990, mean=980, sd=20) ## [1] 0.01760327 What we are likely more interested in, however, is the probability of an adult weighing at least 990 grams. For that, we need the pnorm() function which returns the cumulative density of the normal distribution at a given quantile (for now, think of the quantile as just the cutoff point in adult weights that we are interested in). # what is the probability of observing an adult guillemot that weighs at least 990 grams? pnorm(q = 990, mean = 980, sd = 20, lower.tail=F # we need to specify that we are in the upper tail ) ## [1] 0.3085375 # what about at least 1000 grams? pnorm(q = 1000, mean = 980, sd = 20, lower.tail = F) ## [1] 0.1586553 # what about at least 1020 grams? pnorm(q = 1020, mean = 980, sd = 20, lower.tail = F) ## [1] 0.02275013 As we get into the tails of the distribution, the probability of observing an adult guillemot that is at least that heavy, or heavier, gets increasingly less likely. This brings us to the definition of the p-value: the probability of observing your data, or data more extreme, if the null is true. In our case, we are treating the null hypothesis as being that guillemot weights are drawn from a normal distribution with a mean of 980 and standard deviation of 20. What if instead we our null hypothesis was that the mean was still 980, but that there was a lot more variability in nature, and the standard deviation was 30? The probability of an adult guillemot weighing that much is now much higher (9.1%, as opposed to 0.02%) because there is more spread around the central tendency. pnorm(q=1020, mean=980, sd=30, lower.tail=F) ## [1] 0.09121122 2.6 The Standard Normal Distribution In the real world, we typically do not know what the parameters are underlying our distribution (i.e. we would have know way of knowing what the mean and standard deviation of guillemot weight are). We also often work with data that are normally distributed, but could not have negative values. Remember that the support of the normal distribution is all numbers from \\(-\\infty\\) to \\(+\\infty\\). Adult guillemots, however, cannot have negative weight. Thus, we first need to centralize and standardize the data so that we know what the mean and standard deviation are. Centralizing data means we subtract the mean value of our sample from every single observation, such that we are left with a mean value of 0. Standardizing data is a way of transforming it so that variables are more comparable across different data sources, and also so that we can easily compare standard deviations. To standardize data, we convert it to Z-scores. For each observation \\(x\\), we subtract the mean \\(\\bar{x}\\) and then divide that value by the standard deviation \\(\\sigma\\). The equations to do this for a sample are below; \\(SS\\) stands for the sum of squared residuals. Note that we divide the sum of squares by n-1, rather than n, because we are dealing with a sample rather than population. The degrees of freedom is the number of observations we have, but we lost one degree of freedom when we calculated the sample mean \\(\\bar{x}\\). \\[Z = (x -\\bar{x})/\\sigma\\] \\[\\sigma = \\sqrt{\\sigma^2}\\] \\[\\sigma^2 = \\frac{SS}{(n-1)}\\] \\[SS = \\sum_{i=1}^{n}(x_i - \\bar{x})^2\\] 2.6.1 Functions To standardize our data we need to center it (subtract the mean) and standardize (Z transform) the data. We will create functions in R to do this for us quickly. There are many inbuilt functions in R, such as the ones we have been using already (e.g. rnorm, hist, sum, etc.) and you can also install packages that contain functions which other people have written. You can also write your own custom functions. This is extremely useful when you want to repeat an operation many times without using up many lines of code (the more lines of code, the more likely you are to end up with an error, or something difficult to fix later on). First, we will create a function that calculates just the sum of the squared residuals. To create a function, we need to name it just like an object, but pass to it a function() containing arguments that the function accepts. In this case, it is a function that will take a vector x. Inside the {} brackets we place the instructions for what to do with x and what to return from the function. Only the object that is specified will be returned (i.e. xbar, res, and squares will not be saved to your environment, only ss). sum_squares &lt;- function(x){ xbar &lt;- mean(x) # mean of the sample res &lt;- x - xbar # residuals squares &lt;- res^2 # squared residuals ss &lt;- sum(squares) # sum of the squared residuals return(ss) # what should the function return } We can also nest functions inside other functions, which can make it a lot easier to keep your code neat and tidy. Below is a function to standardize data which uses the other function we just created. standardize &lt;- function(x){ xbar &lt;- mean(x) n &lt;- length(x) ss &lt;- sum_squares(x) # note we call our function from earlier s2 &lt;- ss/(n - 1) # variance = sum of squares divided by n - 1 s &lt;- sqrt(s2) # calculate standard deviation from variance z &lt;- (x - xbar)/s return(z) } # use this function to standardize adult guillemot weights that we generated # earlier and plot the output. how does it differ from the original? std_weights &lt;- standardize(adult_weights) hist(std_weights) # Note that we now have negative weights, however, the interpretation is simply # that these individuals are below the mean, and positive values are above it One of the nice things about the standard normal distribution is that we know that the mean is 0 and the standard deviation is 1. This means that now, when we ask about the probability of observing an adult guillemot with a standardized weight of some value or higher and our null is 0, we are asking if that individual significantly differs from the sample mean. We also know the standard deviations, and instead of having to calculate the integral at different cutoffs, this has already been done. Prior to advances in modern computing, this was done manually, and there were huge lookup tables at the back up statistics textbooks that would tell you the p-value associated with different z-scores. Thankfully, now we have R. Recap Random variables can be described with a probability distribution function. A Bernoulli random variable can be written as \\(y \\sim Bern(p)\\) where \\(p\\) is the probability of success. Only 0 or 1 are possible outcomes. The Bernoulli is a special case of the Binomial distribution with a single draw. A Binomial random variable can be written as \\(y \\sim Bin(N, p)\\) where \\(N\\) is the number of trials. Discrete distributions like the Bernoulli and Binomial have probability mass associated with each potential value and those probabilities must sum to 1. Continuous distributions like the Normal (Gaussian) have probability density associated with each potential value and the area under the curve sums to 1. If we know the probability distribution function, or even if we have empirical data, we can estimate the probability of observing a value, or of observing a value at least that extreme. "],["null-hypothesis-significance-testing.html", "3 Null hypothesis significance testing 3.1 A brief history of statistics and eugenics 3.2 T-distribution and t-tests 3.3 ANOVA 3.4 Type I and Type II error", " 3 Null hypothesis significance testing Conventionally in statistics, people use a threshold (\\(\\alpha\\)) of 0.05 to denote statistical significance. If the probability of observing your data, or data more extreme, is less than the threshold you set, you reject the null hypothesis. This is why you so often see things like \\(p &lt; 0.05\\) in the results section of papers (particularly in older papers, i.e. when you needed a lookup table). Conversely, if your p-value is greater than \\(\\alpha\\), you fail to reject the null hypothesis. You never accept your hypothesis, and you never accept the null hypothesis. You just fail, or do not fail, to reject the null. Why do we conventionally use \\(\\alpha = 0.05\\) as a threshold? “The value for which P=.05, or 1 in 20, is 1.96 or nearly 2; it is convenient to take this point as a limit in judging whether a deviation is to be considered significant or not. Deviations exceeding twice the standard deviation are thus formally regarded as significant.” - Fisher (1925) 3.1 A brief history of statistics and eugenics There are many figures in ecology and evolutionary biology that were giants in their field and helped shape modern science, and who were also extremely problematic individuals (and not just because ‘it was a different time’). Often, the way we learn about these individuals is first through their contributions to the field, and secondarily that they were a ‘bad person’ but that we should still value their intellectual contributions and somehow consider those to be separate from the person. For example, John James Audubon made numerous contributions to ornithology, and also bought and sold enslaved people. You could argue that those are separate aspects of the same person (…you could also argue that he greatly benefited from his status in society as a result of oppressing and enslaving other people and that enabled him to contribute to ornithology, but that is a conversation for a different course…). This is not the case with statistics. Most of modern statistics is built on work done by Sir Francis Galton, Karl Pearson, and Ronald Fisher, all of whom were staunch advocates of eugenics. They also collectively developed the ideas of standard deviation, correlation, regression to the mean, the correlation coefficient, method of moments, \\(\\chi^2\\) test, p-values, principle components analysis, and many other fundamental ideas in statistics. The point is not that these men made exceptional contributions to statistics and also happened to be eugenicists, but rather that they developed statistics to support their eugenicist viewpoints. After all, who could argue with the data they showed to support their view that other races were inferior to white Europeans? Karl Pearson (left) and Sir Francis Galton (right), circa 1910 Many of the concepts we still use to this day in statistics come out of this tradition. For example, even the idea that a population (in the statistical sense) can be described from a single distribution is rooted in eugenics. At the same time Pearson was writing about mixtures of ‘homogeneous groups’ from a mathematical perspective, he was also advocating for colonialism and destruction of what he considered to be ‘inferior races’ because a more ‘homogeneous’ society was better. Pearson in Contributions to the Mathematical Theory of Evolution in 1894 Pearson in National Life from the Standpoint of Science The specific values, thresholds, cutoffs, and conventions that underlie most of null hypothesis significance testing were developed to support a eugenicist agenda. Always remember that they are not mathematically justified or ordained by some law of nature, but rather prescribed by a handful of like-minded eugenicists over a century ago. You should not feel bad about “breaking” their rules from time to time. Also, statistics is constantly evolving, so there are no hard and fast rules to follow. 3.2 T-distribution and t-tests With Z-scores, we are assuming an infinite sample size when we use the standard deviation in the denominator. This is never the case with real data, though as our sample size increases, it becomes less and less important. To account for the reality of small sample sizes, often researchers will use the t distribution instead. The t distribution was developed by William Sealy Gosset, a brewer at Guinness, who essentially wanted to quality check different batches of beer with statistics, but could not use the normal distribution because of the small sample sizes. He published it under the pseudonym Student, which is why it is now called Student’s t-test. # create an object of xvalues over a reasonable range to plot xvals &lt;- seq(from=-4, to=4, by=0.01) # plot the standardized normal (z) distribution over this range plot(dnorm(xvals, mean=0, sd=1), type=&quot;l&quot;) # use the lines() function to add a line with the t distribution over the same range # and assume we have 1 degree of freedom (i.e. a sample size of 2, where df = n - 1) lines(dt(xvals, df=1), lty=2) # as you increase the sample size, and hence the degrees of freedom, the t distribution # approaches, but never reaches, normal (though for most purposes, anything with # a sample size &gt;20 is probably fine to assume normality if it is generally normally # distributed and meets other assumptions) lines(dt(xvals, df=5), lty=2, col=&quot;tomato2&quot;) lines(dt(xvals, df=20), lty=2, col=&quot;tomato2&quot;) We will not spend much time on t-tests because quite frankly they are rarely useful in ecology (and the internet is chock-full of advice), but they can be useful if you want to know if two groups are different from each other. T-tests as illustrated by Alison Horst 3.2.1 Butterfly wing size Let’s say we have two species of butterflies, and we want to know if there is a significant difference in their forewing length. The data we are using for this example come from Daly et al. (2024), who found that warming temperatures can affect development and eventual body size of Arctic butterflies. Here, we are not replicating their full analysis - we are just going to use the data to test if Boloria chariclea is larger than Boloria freija (a fascinating question, I know…). Figure from Daly et al. (2024) showing potential impacts of climate change on body size in ARctic butterflies Because we are working with external data, we need to get it into our environment. There are many different ways to import data depending on the type of file you are reading. In this case, I have exported their data from Dryad and converted it to a comma separated value (.csv) file. Future proofing your data files: When you can use .csv, it is highly, highly recommended because it is future proofed meaning it can be opened by any text editor (unlike e.g. .xlsx). There are of course other options for reading in things like tab-delimited files with read.delim() or Excel spreadsheets with readxl::read.xlsx() and so on, but a simple .csv is the way to go when possible. First, we should read in the data into our environment. Then, it is generally a good idea to explore your data to see what you are working with. The function head will show you, by default, the first six elements of an object (in this case, the first six rows because our object is of the class data.frame). The function colnames will give you the column names (rownames will give you the names of rows). It is also a good idea to look at the structure of your data with the str function which gives you a quick overview of what is contained in the object. For example, with this data, it will tell us we have 2501 observations (i.e. rows of data) of 19 variables (i.e. our columns). It will also tell us all the column names, what type of data are contained in the column (e.g. year is an integer, species is a character, etc.), and give a preview of the first few items in each column. # read in the spreadsheet to our environment dat &lt;- read.csv(&quot;./data/daly2024_butterflysize.csv&quot;) # note that if you have an internet connection, you can also read in data from # remote sources. e.g. I uploaded this spreadsheet as a publicly available # .csv via Google Sheets and you could read it in from there instead of from # a local archive dat &lt;- read.csv(&quot;https://tinyurl.com/daly2024&quot;) # look at the first few rows of data head(dat) ## year barcode box.barcode box envelope Species.determined.by ## 1 1971 UAM100190600 C87023 53-01 UAM100190576 Jayce B. Williamson ## 2 1971 UAM100190625 C87023 53-01 UAM100190573 Jayce B. Williamson ## 3 1971 UAM100190626 C87023 53-01 UAM100190573 Jayce B. Williamson ## 4 1971 UAM100190627 C87023 53-01 UAM100190573 Jayce B. Williamson ## 5 1971 UAM100190639 C87023 53-01 UAM100190570 Jayce B. Williamson ## 6 1971 UAM100190640 C87023 53-01 UAM100190561 Jayce B. Williamson ## species length measurer region sex ## 1 Boloria chariclea 18.70 Abby Blackstone W M ## 2 Boloria chariclea 16.50 Elizabeth Himschoot W M ## 3 Boloria chariclea 16.48 Elizabeth Himschoot W M ## 4 Boloria chariclea 16.41 Elizabeth Himschoot W M ## 5 Boloria chariclea 16.70 Elizabeth Himschoot W M ## 6 Boloria chariclea 18.16 Elizabeth Himschoot W M ## Previous.Summer.Station Spring.weather.station extrap_station extrap_gdd0 ## 1 brrw brrw brrw.nk.ko.at 917.7636 ## 2 brrw brrw brrw.nk.ko.at 917.7636 ## 3 brrw brrw brrw.nk.ko.at 917.7636 ## 4 brrw brrw brrw.nk.ko.at 917.7636 ## 5 brrw brrw brrw.nk.ko.at 917.7636 ## 6 brrw brrw brrw.nk.ko.at 917.7636 ## extrap_gdd5 flight_doy_cutoff prev_summer_gdd spring_gdd ## 1 679.3685 180 61.66 20.28 ## 2 679.3685 180 61.66 20.28 ## 3 679.3685 180 61.66 20.28 ## 4 679.3685 180 61.66 20.28 ## 5 679.3685 180 61.66 20.28 ## 6 679.3685 180 61.66 20.28 # what are all the column names? colnames(dat) ## [1] &quot;year&quot; &quot;barcode&quot; ## [3] &quot;box.barcode&quot; &quot;box&quot; ## [5] &quot;envelope&quot; &quot;Species.determined.by&quot; ## [7] &quot;species&quot; &quot;length&quot; ## [9] &quot;measurer&quot; &quot;region&quot; ## [11] &quot;sex&quot; &quot;Previous.Summer.Station&quot; ## [13] &quot;Spring.weather.station&quot; &quot;extrap_station&quot; ## [15] &quot;extrap_gdd0&quot; &quot;extrap_gdd5&quot; ## [17] &quot;flight_doy_cutoff&quot; &quot;prev_summer_gdd&quot; ## [19] &quot;spring_gdd&quot; # often, it is also a good idea to look at the structure str(dat) ## &#39;data.frame&#39;: 2501 obs. of 19 variables: ## $ year : int 1971 1971 1971 1971 1971 1971 1971 1971 1971 1971 ... ## $ barcode : chr &quot;UAM100190600&quot; &quot;UAM100190625&quot; &quot;UAM100190626&quot; &quot;UAM100190627&quot; ... ## $ box.barcode : chr &quot;C87023&quot; &quot;C87023&quot; &quot;C87023&quot; &quot;C87023&quot; ... ## $ box : chr &quot;53-01&quot; &quot;53-01&quot; &quot;53-01&quot; &quot;53-01&quot; ... ## $ envelope : chr &quot;UAM100190576&quot; &quot;UAM100190573&quot; &quot;UAM100190573&quot; &quot;UAM100190573&quot; ... ## $ Species.determined.by : chr &quot;Jayce B. Williamson&quot; &quot;Jayce B. Williamson&quot; &quot;Jayce B. Williamson&quot; &quot;Jayce B. Williamson&quot; ... ## $ species : chr &quot;Boloria chariclea&quot; &quot;Boloria chariclea&quot; &quot;Boloria chariclea&quot; &quot;Boloria chariclea&quot; ... ## $ length : num 18.7 16.5 16.5 16.4 16.7 ... ## $ measurer : chr &quot;Abby Blackstone&quot; &quot;Elizabeth Himschoot&quot; &quot;Elizabeth Himschoot&quot; &quot;Elizabeth Himschoot&quot; ... ## $ region : chr &quot;W&quot; &quot;W&quot; &quot;W&quot; &quot;W&quot; ... ## $ sex : chr &quot;M&quot; &quot;M&quot; &quot;M&quot; &quot;M&quot; ... ## $ Previous.Summer.Station: chr &quot;brrw&quot; &quot;brrw&quot; &quot;brrw&quot; &quot;brrw&quot; ... ## $ Spring.weather.station : chr &quot;brrw&quot; &quot;brrw&quot; &quot;brrw&quot; &quot;brrw&quot; ... ## $ extrap_station : chr &quot;brrw.nk.ko.at&quot; &quot;brrw.nk.ko.at&quot; &quot;brrw.nk.ko.at&quot; &quot;brrw.nk.ko.at&quot; ... ## $ extrap_gdd0 : num 918 918 918 918 918 ... ## $ extrap_gdd5 : num 679 679 679 679 679 ... ## $ flight_doy_cutoff : int 180 180 180 180 180 180 180 180 180 180 ... ## $ prev_summer_gdd : num 61.7 61.7 61.7 61.7 61.7 ... ## $ spring_gdd : num 20.3 20.3 20.3 20.3 20.3 ... # notice that different columns in the data.frame have different data types # e.g. year is an integer, species is a character (chr), previous summer growing # degree days (prev_summer_gdd) is numeric, etc. But, back to the main point here: doing a quick t-test to see if the two species are different sizes. First, let’s ask the question “Are Boloria chariclea and Boloria freija significantly different in size?”. Notice here that we do not specify which is larger - just that they are different. # create a vector of only the sizes (length) of B freija # and the same for chariclea freija &lt;- dat$length[dat$species==&quot;Boloria freija&quot;] chariclea &lt;- dat$length[dat$species==&quot;Boloria chariclea&quot;] # use a t-test to test if they are different t.test(freija, chariclea) ## ## Welch Two Sample t-test ## ## data: freija and chariclea ## t = -8.4521, df = 1895.9, p-value &lt; 2.2e-16 ## alternative hypothesis: true difference in means is not equal to 0 ## 95 percent confidence interval: ## -0.5197503 -0.3239729 ## sample estimates: ## mean of x mean of y ## 17.46234 17.88420 For identification purposes, it is potentially good to know that the species can be distinguished by size. But imagine telling a field technician that they can tell them apart because they are different sizes. The first question they will ask is which one is bigger. # plot the distribution of sizes for the two species side by side hist(freija, col=&quot;#ffaa0088&quot;, # you can specify colors in RGB format, including opacity border=F, xlim=c(12, 24), # in case the other species needs more room on the axis breaks = 50) hist(chariclea, col=&quot;#ff000044&quot;, border=F, add=T, # add it to the current plot set to TRUE breaks=50) # add a legend to remind ourselves which color is which legend(&quot;topleft&quot;, # placement legend=c(&quot;B. freija&quot;, &quot;B. chariclea&quot;), #text col=c(&quot;#ffaa00&quot;, &quot;#ff0000&quot;), # colors pch=15, # character bty=&quot;n&quot;, # i do not like boxes pt.cex=2) # looks like chariclea is slightly larger Earlier, we did a two-tailed t-test with an \\(\\alpha\\) of 0.05, meaning we only cared if the probability of observing the difference in size that we observed, or an even greater difference, was less than 5%, if the null hypothesis that there was no difference in size was true. But, we could also do a one-tailed test, which would help us answer the question: Is Boloria chariclea larger than Boloria freija? Here, we introduce the idea that we only care about one side (the upper tail) of the t-distribution, not the lower. # NOTE: it matters which species is x and which is y! In this line, we are asking if # chariclea is larger than freija, and the result is yes t.test(x = chariclea, y = freija, alternative = &quot;greater&quot;) ## ## Welch Two Sample t-test ## ## data: chariclea and freija ## t = 8.4521, df = 1895.9, p-value &lt; 2.2e-16 ## alternative hypothesis: true difference in means is greater than 0 ## 95 percent confidence interval: ## 0.3397231 Inf ## sample estimates: ## mean of x mean of y ## 17.88420 17.46234 # but, if we ask in a one-sided test if freija is larger than chariclea, it is no t.test(x = freija, y = chariclea, alternative = &quot;greater&quot;) ## ## Welch Two Sample t-test ## ## data: freija and chariclea ## t = -8.4521, df = 1895.9, p-value = 1 ## alternative hypothesis: true difference in means is greater than 0 ## 95 percent confidence interval: ## -0.5040001 Inf ## sample estimates: ## mean of x mean of y ## 17.46234 17.88420 There are also paired t-tests in which you have a paired set of observations of one thing, and you want to know if they as a group differ from a paired set of observations of another thing. To do this, add the argument paired=T to the t.test function, but each of your vectors will need to actually be paired (i.e. identical length). With our Boloria data, let’s say for some reason we wanted to know if the mean size of Boloria chariclea was larger than Boloria freija every single year in the study, and we wanted to do the comparisons year by year. Not a very sensible question to ask from a biological perspective unless we thought that there were large differences across years, but that’s our example data this week, so we will go with it. First, we will need to do some data wrangling to get our data into the format we want. What we want is a mean value for each year for each species. We can use the aggregate function for this. There are also tidyverse ways to group and summarize data, which we will not talk about in this course because I prefer base. # don&#39;t forget you can look up what arguments a function expects with ? mean_wts &lt;- aggregate(length ~ year + species, data=dat, FUN=mean) # this gives us the mean for each species in each year, but we want it in a # format where each row contains observations for comparison; we can move it to # this form with the xtabs function yearly_wts &lt;- xtabs(length ~ ., data=mean_wts) # now we have vectors of the same length for comparison with a paired t-test # note you do not always need these steps, it depends on your data structure t.test(yearly_wts[,1], yearly_wts[,2], paired=T, alternative = &quot;greater&quot;) ## ## Paired t-test ## ## data: yearly_wts[, 1] and yearly_wts[, 2] ## t = 0.35052, df = 14, p-value = 0.3656 ## alternative hypothesis: true difference in means is greater than 0 ## 95 percent confidence interval: ## -2.307567 Inf ## sample estimates: ## mean of the differences ## 0.5733311 A more practical example of where I’ve used this is to test if there have been phenological shifts in the week of last flight for moths in Ithaca. Each species is only represented once in the historical data, and once in the current data. To see if the whole community is flying later, we used a paired t-test because that way each species was matched up with its own data point historically and now, rather than having a lumped distribution of data historically and a lumped distribution currently. Or, perhaps you have a before and after experimental design setup and want to know if your experimental units have changed in response to a treatment. For example, you have 20 streams and you measure their invertebrate species richness, then introduce standing deadwood to the streams and want to know how it changed the invertebrate community. You do not want to compare the streams to each other - you want to compare each to their previous invertebrate community, so you have pairwise samples. 3.3 ANOVA What if instead of comparing two groups, we wanted to compare three or more groups? That’s called an analysis of variance (ANOVA). The dataset we read in earlier actually has three different species and also includes Colias hecla. For no real scientific reason whatsoever, we will ask the question: are these three butterfly species different in size? # we use the aov function, and here we will specify our data a bit differently # instead of putting in two separate species vectors, we will use the tilde ~ # to indicate that we want to model the response variable (length) over the # categorical groups (species) in our dataset (dat) size_mod &lt;- aov(length ~ species, data = dat) # we saved this output to an object, because typically you want to actually be # able to work with your model output, not just run it once and do nothing # the summary() function applied to the model will give us the results summary(size_mod) ## Df Sum Sq Mean Sq F value Pr(&gt;F) ## species 2 10964 5482 4086 &lt;2e-16 *** ## Residuals 2497 3350 1 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## 1 observation deleted due to missingness The model summary tells us that there are significant differences between the three species, but do we know if that’s because one is much bigger (or smaller) than the others, or that they are all different from each other. To see what differences resulted in a significant difference in the overall model, we can use Tukey’s Honestly Significant Differences, which will give us the difference between the groups, the lower and upper bound of the difference, and the p-value associated with that difference. Note that if the lower and upper bounds of the difference overlap zero, then we would not know if there was a difference between the two groups. That is another way to think about p-values and significance (assuming 0 difference is your null hypothesis). These upper and lower values are confidence intervals. TukeyHSD(size_mod) ## Tukey multiple comparisons of means ## 95% family-wise confidence level ## ## Fit: aov(formula = length ~ species, data = dat) ## ## $species ## diff lwr upr p adj ## Boloria freija-Boloria chariclea -0.4218616 -0.5462763 -0.297447 0 ## Colias hecla-Boloria chariclea 4.6964363 4.5547907 4.838082 0 ## Colias hecla-Boloria freija 5.1182979 4.9758238 5.260772 0 3.4 Type I and Type II error When doing null hypothesis significance testing (NHST) with different thresholds of \\(\\alpha\\), remember that we only either reject or fail to reject the null. But, we also know that our p-values are the probability of observing our data, or data more extreme, if the null were true. This means that there is always a non-zero chance of observing your data and rejecting the null even if the null is true. If we reject the null when it is true, this is a false positive (i.e. we have identified a relationship or difference in our data that does not actually exist) or a type I error. The probability of making a type I error is our \\(\\alpha\\) threshold. When setting a threshold for \\(\\alpha\\), what we are really asking ourselves is ‘what probability am I okay with as an upper bound for claiming something is true when it is not?’ Conversely, a type II error is when we fail to reject the null even though the null is false (i.e. a false negative, we say there is no difference when there really is). The probability of making a type II error is denoted \\(\\beta\\) and represents the chance that we will miss an association or effect that truly exists. \\(1 - \\beta\\) is the ‘power’ of our test. Type I vs Type II error When planning research, it is helpful to know if you will likely be able to detect the effect that you expect exists in the population, which partly depends on sample size. Power analysis is an approach to estimate what sample size you will need, what effect is possible to detect with your data, or what power you have to detect an observed effect given your data. Note that the last one is typically done post-hoc to ‘explain’ negative results and is bad scientific practice; do not do post-hoc power analyses. For t-tests, there is a built-in function in R to do power analysis called power.t.test. Using this function, what is the chance, given our data, that we will make a type II error when comparing Boloria chariclea to B. freija wing length? N &lt;- nrow(dat)/2 sd &lt;- sd(dat$length) alpha &lt;- 0.05 meandiff &lt;- mean(dat$length[dat$species==&quot;Boloria chariclea&quot;], na.rm=T) - mean(dat$length[dat$species==&quot;Boloria freija&quot;], na.rm=T) power.t.test(n = N, delta = meandiff, sig.level = alpha) ## ## Two-sample t test power calculation ## ## n = 1250.5 ## delta = 0.4218616 ## sd = 1 ## sig.level = 0.05 ## power = 1 ## alternative = two.sided ## ## NOTE: n is number in *each* group # What if we only sampled 20 individuals of each species? power.t.test(n = 20, delta = meandiff, sig.level = alpha) ## ## Two-sample t test power calculation ## ## n = 20 ## delta = 0.4218616 ## sd = 1 ## sig.level = 0.05 ## power = 0.2547824 ## alternative = two.sided ## ## NOTE: n is number in *each* group Power analysis can be especially useful if you are doing destructive or lethal sampling because you can use it to estimate the minimum number of samples or individuals you need to sacrifice to detect different thresholds. For instance, let’s say we were collecting hummingbirds from high elevation and low elevation sites, bringing them to a common location, measuring their metabolic rates, and then sacrificing them to weigh their internal organs. Naturally, we would want to sacrifice as few individuals as possible. If we wanted 80% power to detect an effect size difference of 0.2 between individuals from low and high elevation sites with a probability of a false positive of 5%, how many hummingbirds would we need? What if we weren’t sure about the effect and we thought it might be between 0.1 and 0.8? possible_effects &lt;- seq(0.1, 0.8, by=0.05) sample_size &lt;- c() for(i in 1:length(possible_effects)){ power1 &lt;- power.t.test(n = NULL, delta=possible_effects[i], sd=1, sig.level=0.05, power=0.20) sample_size[i] &lt;- power1$n } plot(sample_size ~ possible_effects, pch=19, las=1) "],["inference.html", "4 Inference 4.1 Lyrebird song repertoire", " 4 Inference One of the goals of statistics is to make inference about the natural world. We make inference based on observations taken on our sample. In the Arctic butterfly example, the sample was the 2501 individual butterflies that Daly et al. measured. The observations are not just the length of each individual, but also their species identity (and lots of other data we did not dig into yet). In statistics, we assume that our sample is representative of the population. This does not mean a population in the ecological or evolutionary sense, but rather the universe that we are trying to make inference about. The way we defined the population for the t-tests was at the level of the species, i.e. we assumed the 968 Boloria chariclea individuals represented that species for the purposes of inference. The properties we measure on our sample are the variables at our disposal, i.e. species, length, growing degree days, region, etc. each matched to a single observation. When making inference, we can assume that the sample mean is an unbiased estimator of the population mean if: Individuals are randomly selected from the population. i.e. we did not just choose the closest butterflies, or the prettiest butterflies, or the slowest butterflies, etc. Observations are independent of each other. i.e. not all the butterflies came from the same site, or the same person measured one species and a different person measured the other species, etc. This assumption is rarely met in ecology (…including in our t-tests that we did above). The population parameter we are interested in can be described as a random variable, meaning that it comes from a probability distribution. In calculating the sample mean \\(\\bar{x}\\) and sample variance \\(s^2\\), we are trying to make inference about the population mean \\(\\mu\\) and variance \\(\\sigma^2\\). The latter are our population parameters. This is why you will sometimes hear the phrase ‘parametric’ when referring to statistics or modeling when there are parameters, whereas ‘non-parametric’ stats do not try to estimate or make inference about population parameters. Frequentist statistics assumes that population parameters are fixed, and the data are random. Bayesians, on the other hand, assume that the data are fixed and the parameters are random. In other words, frequentists assume that the world works in a certain way and we observe it imperfectly each time we collect a sample, while Bayesians tend to think more in terms of probability distributions that could have generated the data that we observed. We can simulate data based on fixing the underlying population parameters, then seeing how closely our sample approximates the population. 4.1 Lyrebird song repertoire Lyrebirds can learn new songs from their environment, making it possible for individuals who have been exposed to more songs to have a larger song repertoire. As a motivating example, let’s pretend we are trying to understand the song repertoire of a population of lyrebirds (in this case, in both the ecological and statistical sense!). For the sake of this example, we will say that we know that the true population mean number of songs that lyrebirds in the population know is 38.1. We will also fix the standard deviation to 4. These are our population parameters. # define our population parameters true_mean &lt;- 38.1 true_sd &lt;- 4 # Now, we go out into the forest and sample some individuals from the population # by recording the number of songs they know. Let&#39;s say we observed 10 individuals. n.obs &lt;- 10 song_rep &lt;- rnorm(n.obs, true_mean, sd = true_sd) # What is the mean estimated from this sample? mean(song_rep) ## [1] 37.05111 # How close it is to the true population mean? xvals &lt;- seq(10, 50, 0.01) # set up a reasonable range of x values for plotting purposes # first, plot the *true* probability density over this range of values plot(x=xvals, y=dnorm(xvals, mean = true_mean, sd = true_sd), type=&quot;l&quot;, ylab=&quot;Density&quot;, xlab=&quot;Value&quot;, las=1) abline(v=mean(song_rep), lty=2) # sample mean from our 10 observations abline(v=true_mean, lty=1) # true mean # What if we sampled a different 10 individuals? abline(v=mean(rnorm(n.obs, true_mean, true_sd)), lty=2, col=&quot;tomato2&quot;) # a new sample "],["the-law-of-large-numbers.html", "5 The law of large numbers 5.1 Confidence intervals 5.2 Lyrebird population differences 5.3 Lyrebird mating success and the sampling distribution 5.4 Credible intervals", " 5 The law of large numbers As our sample size increases, the sample mean \\(\\bar{x}\\) will more closely approximate the population mean \\(\\mu\\). To demonstrate this, we are going to use something called a for loop. A for loop allows us to iterate a process. One way to think about a for loop is as a set of instructions for an extremely literal toddler. Pretend you have this bowl of fruit, and you need the toddler to pick up a piece of fruit, bring it to the kitchen, wash it, put it into a new bowl, and then repeat this for the next fruit. They can only carry one fruit at a time. How we would write this as a for loop is to first specify what they will be working with, i.e. “for each fruit in the fruit bowl” is written as for(fruit in fruit_bowl). Then, the next step would be “take fruit to kitchen” which might be to_kitchen(fruit), and so on. In the code below, we will use the print() function instead because there is no such thing as a to_kitchen function (unless you write it yourself). fruit_bowl &lt;- c(&quot;kiwi&quot;, &quot;banana&quot;, &quot;apple&quot;, &quot;pomegranate&quot;, &quot;pear&quot;) for(fruit in fruit_bowl){ print(paste(&quot;pick up&quot;, fruit)) # paste is a function that concatenates strings print(paste(&quot;take&quot;, fruit, &quot;to kitchen&quot;)) print(paste(&quot;wash&quot;, fruit)) print(paste(&quot;set&quot;, fruit, &quot;in new bowl&quot;)) } ## [1] &quot;pick up kiwi&quot; ## [1] &quot;take kiwi to kitchen&quot; ## [1] &quot;wash kiwi&quot; ## [1] &quot;set kiwi in new bowl&quot; ## [1] &quot;pick up banana&quot; ## [1] &quot;take banana to kitchen&quot; ## [1] &quot;wash banana&quot; ## [1] &quot;set banana in new bowl&quot; ## [1] &quot;pick up apple&quot; ## [1] &quot;take apple to kitchen&quot; ## [1] &quot;wash apple&quot; ## [1] &quot;set apple in new bowl&quot; ## [1] &quot;pick up pomegranate&quot; ## [1] &quot;take pomegranate to kitchen&quot; ## [1] &quot;wash pomegranate&quot; ## [1] &quot;set pomegranate in new bowl&quot; ## [1] &quot;pick up pear&quot; ## [1] &quot;take pear to kitchen&quot; ## [1] &quot;wash pear&quot; ## [1] &quot;set pear in new bowl&quot; A more common way to use for loops is with an index. Rather than setting up our fruit bowl in advance, we instead have a list of fruits and we can subset each one based on numbers. i.e. fruit_bowl[1] is \"kiwi\". What is nice about this is you can combine different vectors and subset them by the same index. fruits &lt;- c(&quot;apples&quot;, &quot;bananas&quot;, &quot;grapes&quot;) # here we use i as a placeholder to stand for index for(i in 1:3){ print(fruits[i]) } ## [1] &quot;apples&quot; ## [1] &quot;bananas&quot; ## [1] &quot;grapes&quot; # add in a second vector of adjectives adjectives &lt;- c(&quot;decent&quot;, &quot;disgusting&quot;, &quot;just okay&quot;) for(i in 1:3){ print(paste(fruits[i], &quot;are&quot;, adjectives[i])) } ## [1] &quot;apples are decent&quot; ## [1] &quot;bananas are disgusting&quot; ## [1] &quot;grapes are just okay&quot; You can also nest for loops within each other, but that is for a different day. What we want to use for loops for right now is to combine them with a probability density function (rnorm) to visualize the central limit theorem. # Using the same population parameters as above, if we were to sample more # individuals, would we converge towards the true mean? n.samples &lt;- 200 more_samples &lt;- rnorm(n.samples, true_mean, sd = true_sd) running_avg &lt;- c() for(i in 1:n.samples){ running_avg[i] &lt;- mean(more_samples[1:i]) } plot(running_avg, type=&quot;l&quot;, lwd=2, ylim=c(30, 45), ylab=&quot;Mean song repertoire&quot;, xlab=&quot;Number of samples&quot;, axes=F, las=1) axis(1); axis(2) abline(h=true_mean, lty=2) 5.1 Confidence intervals Frequentists. Remember that in a frequentist approach to inference, the population parameters are fixed and the data are random. This means that sometimes our sample will not contain the true population mean. Using the standard deviation of our data and our significance threshold, we can generate a confidence interval to describe this process. Often, people will use a 95% confidence interval because it matches the convention of \\(\\alpha = 0.05\\). What a 95% confidence interval tells us is that 95% of the time, if you go out and calculate an interval in this way, it will contain the true population mean. plot(x=xvals, y=dnorm(xvals, mean = true_mean, sd = true_sd), type=&quot;l&quot;, ylab=&quot;Density&quot;, xlab=&quot;Value&quot;, axes=F) axis(1); axis(2) ci &lt;- c(true_mean-1.96*true_sd, true_mean+1.96*true_sd) ci.vals &lt;- seq(ci[1], ci[2], 0.1) polygon(x = c(ci.vals, rev(ci.vals)), y = c(dnorm(ci.vals, true_mean, true_sd), rep(0, length(ci.vals))), col=&quot;#bb5406bb&quot;, border=F) text(ci[1], 0, &quot;μ - 1.96*σ&quot;, cex=1.5) text(ci[2], 0, &quot;μ + 1.96*σ&quot;, cex=1.5) That means if you went out and collected observations on a sample (e.g. 10 lyrebirds) and then repeated that 100 times (i.e. 100 replicates of each sample, so 1000 total individual butterflies, but observed independently in 100 different replicates, each with a sample size of 10), you would expect that 95 out of those 100 confidence intervals would include the true population mean. 5.2 Lyrebird population differences Now let’s assume we have two different lyrebird populations, and we want to know if they differ in their population-level song repertoires. We will assume one population is in a natural habitat where they are only exposed to natural sounds, such as other bird calls, water, wind, etc. The other is in a degraded woodland near human population centers and is also exposed to anthropogenic noises, which we might expect would increase the mean song repertoire for that population. nat_pop_mean &lt;- 38.1 nat_sd &lt;- 2 urb_pop_mean &lt;- 42.4 urb_sd &lt;- 4 true_diff &lt;- urb_pop_mean - nat_pop_mean n.obs &lt;- 30 # we observe 30 individuals in each population n.reps &lt;- 100 # For each replication, we will use a one-sided t-test to test if the urban # population has a larger song repertoire than the natural population # set up placeholder objects rep_diff &lt;- rep_low &lt;- rep_up &lt;- contains_true &lt;- c() for(i in 1:n.reps){ nat_pop &lt;- rnorm(n.obs, nat_pop_mean, nat_sd) urb_pop &lt;- rnorm(n.obs, urb_pop_mean, urb_sd) mod1 &lt;- t.test(urb_pop, nat_pop) rep_diff[i] &lt;- mean(urb_pop) - mean(nat_pop) rep_low[i] &lt;- mod1$conf.int[1] rep_up[i] &lt;- mod1$conf.int[2] contains_true[i] &lt;- as.logical(rep_low[i]&lt;true_diff &amp; rep_up[i]&gt;true_diff) } plot(rep_diff ~ seq(1, n.reps, 1), ylim=c(-10, 10), col=c(&quot;red&quot;, &quot;black&quot;)[factor(contains_true)]) arrows(x0=seq(1, n.reps, 1), y0=rep_low, y1=rep_up, code=3, angle=90, length=0.05, col=c(&quot;red&quot;, &quot;black&quot;)[factor(contains_true)]) abline(h=true_diff) table(contains_true) ## contains_true ## FALSE TRUE ## 7 93 5.3 Lyrebird mating success and the sampling distribution The law of large numbers does not just apply to the normal distribution, or even just to continuous distributions for that matter. The sampling distribution for any parameter will converge on the true population value with enough samples. It is important to keep the sampling distribution and sample distribution separate; the sampling distribution is the distribution of statistics calculated from a bunch of different samples, whereas the sample distribution is the distribution of a single sample. Let’s assume each male lyrebird in our population has a 0.25 probability of mating success, we observe 20 males in each of our samples, and we replicate our sample 100 times. sample1 &lt;- rbinom(20, 1, 0.25) # this is our sample hist(sample1) # this is our sample distribution # we do not have a sampling distribution yet; for that we need replicates rep_means &lt;- c() for(i in 1:100){ rep_means[i] &lt;- mean(sample(rbinom(20, 1, 0.25))) } hist(rep_means) # this is our sampling distribution 5.4 Credible intervals We will get to Bayesian statistics later in the semester, but for now, know that the Bayesian equivalent of a confidence interval is called a credible interval. The interpretation of a 95% Bayesian credible interval is that there is a 95% probability that the true population mean lies within that interval. This is frequently mixed up in the literature and frequentists often describe confidence intervals as if they are credible intervals but this is 100% not true. Bayesians. "],["visualizing-group-differences.html", "6 Visualizing group differences 6.1 Boxplots 6.2 Beehive plots 6.3 Density plots 6.4 Raincloud plots 6.5 Paired slopes", " 6 Visualizing group differences 6.1 Boxplots 6.2 Beehive plots 6.3 Density plots 6.4 Raincloud plots 6.5 Paired slopes "],["404.html", "Page not found", " Page not found The page you requested cannot be found (perhaps it was moved or renamed). You may want to try searching to find the page's new location, or use the table of contents to find the page you are looking for. "]]
